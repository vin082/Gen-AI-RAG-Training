{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Your First RAG ChatBot with LangChain\n",
    "## Part 5a: From Documents to Intelligent Q&A in 30 Minutes\n",
    "\n",
    "**Welcome to the finale of our LangChain RAG series!** ðŸŽ‰\n",
    "\n",
    "In the previous articles, we learned:\n",
    "- **Part 1**: [Document Loading](link) - Getting data into your system\n",
    "- **Part 2**: [Document Splitting](blog_document_splitting.md) - Optimal chunking strategies\n",
    "- **Part 3**: [Vector Embeddings](blog_vector_embeddings.md) - Semantic understanding\n",
    "- **Part 4**: [Vector Databases](link) - Storage and retrieval at scale\n",
    "\n",
    "**Now we bring it ALL together!** ðŸš€\n",
    "\n",
    "In this notebook, you'll build a working RAG chatbot that:\n",
    "- âœ… Answers questions about YOUR documents\n",
    "- âœ… Shows sources for answers\n",
    "- âœ… Runs completely locally (free!)\n",
    "- âœ… Takes ~30 minutes to set up\n",
    "\n",
    "---\n",
    "\n",
    "## What is RAG Again?\n",
    "\n",
    "**RAG (Retrieval Augmented Generation)** = Smart Q&A over your documents\n",
    "\n",
    "**The Problem:**\n",
    "- LLMs don't know about YOUR specific documents\n",
    "- They can't access real-time or proprietary information\n",
    "- They \"hallucinate\" when they don't know answers\n",
    "\n",
    "**The Solution (RAG):**\n",
    "1. User asks a question\n",
    "2. System retrieves relevant document chunks\n",
    "3. LLM generates answer using those chunks\n",
    "4. Result: Accurate, grounded responses with sources!\n",
    "\n",
    "---\n",
    "\n",
    "**ðŸ’¡ By the end:** You'll have a chatbot answering questions about any PDF, website, or text you provide!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture Overview\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Documents  â”‚ (PDFs, websites, text)\n",
    "â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜\n",
    "       â”‚\n",
    "       â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚   Loader    â”‚ (Read documents)\n",
    "â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜\n",
    "       â”‚\n",
    "       â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Splitter   â”‚ (Chunk into pieces)\n",
    "â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜\n",
    "       â”‚\n",
    "       â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ Embeddings  â”‚ (Convert to vectors)\n",
    "â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜\n",
    "       â”‚\n",
    "       â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ Vector DB   â”‚ (Store in Chroma)\n",
    "â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜\n",
    "       â”‚\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚             â”‚\n",
    "â–¼             â–¼\n",
    "User â”€â”€â–º Retriever â”€â”€â–º LLM â”€â”€â–º Answer\n",
    "Question     (Find       (Generate\n",
    "             relevant)   response)\n",
    "```\n",
    "\n",
    "**Simple, right?** Let's build it step by step!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install all required packages\n",
    "pip install -q langchain langchain-community langchain-chroma\n",
    "pip install -q langchain-huggingface sentence-transformers\n",
    "pip install -q langchain-openai  # Optional: for OpenAI models\n",
    "pip install -q pypdf  # For PDF loading\n",
    "pip install -q chromadb  # Vector database\n",
    "\n",
    "print(\"âœ“ All packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnostic: Check Python, Torch, GPU, and DLL issues\n",
    "import sys, os, subprocess, traceback\n",
    "print('Python executable:', sys.executable)\n",
    "print('Python version:', sys.version.replace('\\n',' '))\n",
    "print('64-bit Python?:', sys.maxsize > 2**32)\n",
    "print('\\nFirst entries of sys.path:')\n",
    "for p in sys.path[:10]:\n",
    "    print('  ', p)\n",
    "\n",
    "print('\\n--- pip show torch (for this Python binary) ---')\n",
    "subprocess.run([sys.executable, '-m', 'pip', 'show', 'torch'])\n",
    "\n",
    "print('\\n--- pip list (filtered for torch packages) ---')\n",
    "subprocess.run([sys.executable, '-m', 'pip', 'list', '--format=columns'])\n",
    "\n",
    "print('\\n--- conda list torch (if conda available) ---')\n",
    "try:\n",
    "    subprocess.run(['conda', 'list', 'torch'])\n",
    "except Exception as e:\n",
    "    print('conda not available or error:', e)\n",
    "\n",
    "print('\\n--- nvidia-smi output (if NVIDIA GPU/drivers installed) ---')\n",
    "try:\n",
    "    subprocess.run(['nvidia-smi'])\n",
    "except Exception as e:\n",
    "    print('nvidia-smi not found or error:', e)\n",
    "\n",
    "print('\\n--- Attempt to import torch and inspect error (if any) ---')\n",
    "try:\n",
    "    import torch\n",
    "    print('torch import succeeded')\n",
    "    print('torch version:', getattr(torch, '__version__', 'unknown'))\n",
    "    print('torch cuda version:', getattr(torch.version, 'cuda', None))\n",
    "    print('cuda available:', torch.cuda.is_available())\n",
    "    print('torch lib location:', getattr(torch, '__file__', None))\n",
    "except Exception as e:\n",
    "    print('Import torch failed (traceback below):')\n",
    "    traceback.print_exc()\n",
    "    print()\n",
    "    print('Common fixes:')\n",
    "    print('- Install matching PyTorch build for your CUDA driver or install CPU-only PyTorch')\n",
    "    print('- Update GPU drivers / CUDA toolkit (if using GPU)')\n",
    "    print('- Install the Microsoft Visual C++ 2015-2022 Redistributable (x64)')\n",
    "    print('- Try creating a fresh conda env and install CPU-only PyTorch to isolate the issue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: API Keys Setup\n",
    "\n",
    "**Two options for LLM:**\n",
    "1. **OpenAI** (paid, best quality) - Requires API key\n",
    "2. **Local LLM** (free, good enough) - No API key needed\n",
    "\n",
    "We'll show both approaches!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Langchain Training\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Configuration ready\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import create_agent\n",
    "import os\n",
    "\n",
    "# Option 1: Use OpenAI (uncomment if you have API key)\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Option 2: We'll use HuggingFace (free, local)\n",
    "# No API key needed!\n",
    "\n",
    "print(\"âœ“ Configuration ready\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Step 1: Load Documents ðŸ“„\n",
    "\n",
    "First, we need documents to answer questions about. We'll use multiple sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option A: Load from Sample Text (Quick Start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Loaded 5 sample documents\n",
      "\n",
      "Sample content: LangChain is a framework for developing applications powered by language models. \n",
      "        It enables...\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "# Sample documents about LangChain and RAG\n",
    "sample_docs = [\n",
    "    Document(\n",
    "        page_content=\"\"\"LangChain is a framework for developing applications powered by language models. \n",
    "        It enables applications that are context-aware and can reason about information. LangChain provides \n",
    "        tools for document loading, text splitting, embeddings, vector stores, and chains for building \n",
    "        complex LLM applications.\"\"\",\n",
    "        metadata={\"source\": \"langchain_intro.txt\", \"topic\": \"LangChain Overview\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"\"\"Retrieval Augmented Generation (RAG) is a technique that combines information retrieval \n",
    "        with text generation. It works by first retrieving relevant documents from a knowledge base, then \n",
    "        passing those documents to a language model to generate accurate, grounded responses. This prevents \n",
    "        hallucinations and allows LLMs to answer questions about specific, up-to-date information.\"\"\",\n",
    "        metadata={\"source\": \"rag_explanation.txt\", \"topic\": \"RAG\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"\"\"Vector embeddings are numerical representations of text that capture semantic meaning. \n",
    "        Similar texts produce similar embedding vectors. Common embedding models include OpenAI's text-embedding-3-small, \n",
    "        sentence-transformers from HuggingFace like all-MiniLM-L6-v2, and Google's text-embedding-004. \n",
    "        Embeddings typically have 384 to 1536 dimensions.\"\"\",\n",
    "        metadata={\"source\": \"embeddings_guide.txt\", \"topic\": \"Embeddings\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"\"\"Chroma is an open-source embedding database designed for AI applications. \n",
    "        It provides fast similarity search, easy integration with LangChain, and persistent storage. \n",
    "        Chroma supports metadata filtering and is perfect for building RAG applications. It can run \n",
    "        locally or in client-server mode.\"\"\",\n",
    "        metadata={\"source\": \"chroma_db.txt\", \"topic\": \"Vector Databases\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"\"\"Document splitting is crucial for RAG systems. The RecursiveCharacterTextSplitter \n",
    "        is recommended as it intelligently splits on paragraphs, sentences, and words in order. \n",
    "        Optimal chunk size is typically 500-1000 characters with 10-20% overlap. The overlap ensures \n",
    "        context isn't lost at chunk boundaries.\"\"\",\n",
    "        metadata={\"source\": \"splitting_guide.txt\", \"topic\": \"Document Splitting\"}\n",
    "    ),\n",
    "]\n",
    "\n",
    "print(f\"âœ“ Loaded {len(sample_docs)} sample documents\")\n",
    "print(f\"\\nSample content: {sample_docs[0].page_content[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option B: Load from PDF (Your Own Documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Langchain Training\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Loaded 117 pages from PDF\n"
     ]
    }
   ],
   "source": [
    "# Uncomment to load your own PDF\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "\n",
    "loader = PyMuPDFLoader(\"data/AI_Agents.pdf\")\n",
    "sample_docs = loader.load()\n",
    "print(f\"âœ“ Loaded {len(sample_docs)} pages from PDF\")\n",
    "\n",
    "#print(\"ðŸ’¡ To use your own PDF: uncomment above and provide PDF path\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option C: Load from Website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to load from a website\n",
    "# from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "# loader = WebBaseLoader(\"https://python.langchain.com/docs/get_started/introduction\")\n",
    "# sample_docs = loader.load()\n",
    "# print(f\"âœ“ Loaded content from website\")\n",
    "\n",
    "print(\"ðŸ’¡ To load from web: uncomment above and provide URL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Step 2: Split Documents into Chunks ðŸ”ª\n",
    "\n",
    "We split documents into smaller chunks for better retrieval precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Split 5 documents into 5 chunks\n",
      "\n",
      "Sample chunk:\n",
      "Content: LangChain is a framework for developing applications powered by language models. \n",
      "        It enables applications that are context-aware and can reason about information. LangChain provides \n",
      "        tools for document loading, text splitting, embeddings, vector stores, and chains for building \n",
      "        complex LLM applications.\n",
      "Metadata: {'source': 'langchain_intro.txt', 'topic': 'LangChain Overview'}\n",
      "Chunk size: 328 characters\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Create text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,        # Size of each chunk\n",
    "    chunk_overlap=50,      # Overlap between chunks (preserves context)\n",
    "    length_function=len,   # How to measure chunk size\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]  # Split on paragraphs, then sentences, then words\n",
    ")\n",
    "\n",
    "# Split the documents\n",
    "splits = text_splitter.split_documents(sample_docs)\n",
    "\n",
    "print(f\"âœ“ Split {len(sample_docs)} documents into {len(splits)} chunks\")\n",
    "print(f\"\\nSample chunk:\")\n",
    "print(f\"Content: {splits[0].page_content}\")\n",
    "print(f\"Metadata: {splits[0].metadata}\")\n",
    "print(f\"Chunk size: {len(splits[0].page_content)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ðŸ’¡ Why splitting matters:**\n",
    "- Too large: Retrieves irrelevant info, wastes LLM context\n",
    "- Too small: Loses context, poor answers\n",
    "- Just right: Precise retrieval, better answers!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Step 3: Create Embeddings ðŸ§®\n",
    "\n",
    "Convert text chunks into vector embeddings for semantic search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Embeddings model loaded\n",
      "Model: all-MiniLM-L6-v2\n",
      "Embedding dimensions: 384\n",
      "Sample values: [-0.04055781289935112, 0.013319483026862144, 0.019409120082855225, 0.0048216828145086765, -0.06119084730744362]\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# Initialize embeddings model (free, runs locally)\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"all-MiniLM-L6-v2\",  # Fast, 384 dimensions\n",
    "    #model_name=\"all-mpnet-base-v2\",\n",
    "    model_kwargs={'device': 'cpu'}  # Use 'cuda' if you have GPU\n",
    ")\n",
    "\n",
    "# Test the embeddings\n",
    "test_text = \"What is LangChain?\"\n",
    "test_embedding = embeddings.embed_query(test_text)\n",
    "\n",
    "print(f\"âœ“ Embeddings model loaded\")\n",
    "print(f\"Model: all-MiniLM-L6-v2\")\n",
    "\n",
    "print(f\"Embedding dimensions: {len(test_embedding)}\")\n",
    "print(f\"Sample values: {test_embedding[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Alternative: Use OpenAI Embeddings (if you have API key)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Using OpenAI embeddings\n",
      "ðŸ’¡ OpenAI embeddings are higher quality but cost ~$0.02 per 1M tokens\n"
     ]
    }
   ],
   "source": [
    "# Uncomment if using OpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "print(\"âœ“ Using OpenAI embeddings\")\n",
    "\n",
    "print(\"ðŸ’¡ OpenAI embeddings are higher quality but cost ~$0.02 per 1M tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Step 4: Create Vector Store with Chroma ðŸ—„ï¸\n",
    "\n",
    "Store embeddings in Chroma for fast similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating vector store...\n",
      "âœ“ Vector store created with 5 chunks\n",
      "âœ“ Persisted to: ./chroma_rag_db\n",
      "\n",
      "Test Query: 'What is RAG?'\n",
      "Top result: Retrieval Augmented Generation (RAG) is a technique that combines information retrieval \n",
      "        wit...\n"
     ]
    }
   ],
   "source": [
    "from langchain_chroma import Chroma\n",
    "import shutil\n",
    "\n",
    "# Clean up any existing database\n",
    "shutil.rmtree(\"./chroma_rag_db\", ignore_errors=True)\n",
    "\n",
    "# Create vector store\n",
    "print(\"Creating vector store...\")\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=\"./chroma_rag_db\",  # Persist to disk\n",
    "    collection_name=\"rag_demo\"\n",
    ")\n",
    "\n",
    "print(f\"âœ“ Vector store created with {len(splits)} chunks\")\n",
    "print(f\"âœ“ Persisted to: ./chroma_rag_db\")\n",
    "\n",
    "# Test retrieval\n",
    "test_query = \"What is RAG?\"\n",
    "test_results = vectorstore.similarity_search(test_query, k=2)\n",
    "\n",
    "print(f\"\\nTest Query: '{test_query}'\")\n",
    "print(f\"Top result: {test_results[0].page_content[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ðŸ’¡ What just happened:**\n",
    "1. Each chunk was converted to a 384-dimensional vector\n",
    "2. Vectors stored in Chroma with metadata\n",
    "3. Chroma created an index for fast similarity search\n",
    "4. Data persisted to disk (can reload later!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Step 5: Create a Retriever ðŸ”\n",
    "\n",
    "Retriever finds the most relevant chunks for a query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Retriever created with MMR search\n",
      "  - Returns: 3 most relevant chunks\n",
      "  - Strategy: MMR (diverse, non-redundant results)\n",
      "\n",
      "Test Query: 'What are AI Agents?'\n",
      "Retrieved 3 chunks:\n",
      "\n",
      "1. Retrieval Augmented Generation (RAG) is a technique that combines information re...\n",
      "   Source: rag_explanation.txt\n",
      "\n",
      "2. Chroma is an open-source embedding database designed for AI applications. \n",
      "     ...\n",
      "   Source: chroma_db.txt\n",
      "\n",
      "3. Vector embeddings are numerical representations of text that capture semantic me...\n",
      "   Source: embeddings_guide.txt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create retriever from vector store\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"mmr\",  # Maximum Marginal Relevance (diverse results)\n",
    "    search_kwargs={\n",
    "        \"k\": 3,          # Return top 3 chunks\n",
    "        \"fetch_k\": 10    # Fetch 10 candidates for MMR selection\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"âœ“ Retriever created with MMR search\")\n",
    "print(\"  - Returns: 3 most relevant chunks\")\n",
    "print(\"  - Strategy: MMR (diverse, non-redundant results)\")\n",
    "\n",
    "# Test retrieval\n",
    "test_query = \"What are AI Agents?\"\n",
    "retrieved_docs = retriever.invoke(test_query)\n",
    "#retrieved_docs = retriever.get_relevant_documents(test_query)\n",
    "\n",
    "print(f\"\\nTest Query: '{test_query}'\")\n",
    "print(f\"Retrieved {len(retrieved_docs)} chunks:\\n\")\n",
    "for i, doc in enumerate(retrieved_docs, 1):\n",
    "    print(f\"{i}. {doc.page_content[:80]}...\")\n",
    "    print(f\"   Source: {doc.metadata.get('source', 'N/A')}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "\n",
    "@tool(response_format=\"content_and_artifact\")\n",
    "def retrieve_context(query: str):\n",
    "    \"\"\"Retrieve information to help answer a query.\"\"\"\n",
    "    retrieved_docs = vectorstore.similarity_search(query, k=2)\n",
    "    serialized = \"\\n\\n\".join(\n",
    "        (f\"Source: {doc.metadata}\\nContent: {doc.page_content}\")\n",
    "        for doc in retrieved_docs\n",
    "    )\n",
    "    return serialized, retrieved_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ðŸ’¡ Why MMR?**\n",
    "- Standard search might return 3 very similar chunks\n",
    "- MMR balances relevance + diversity\n",
    "- Result: Better coverage, less redundancy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Step 6: Set Up the LLM ðŸ¤–\n",
    "\n",
    "Choose your LLM: OpenAI (paid) or HuggingFace (free)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option A: OpenAI (Best Quality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Using OpenAI GPT-4o-mini\n",
      "ðŸ’¡ OpenAI provides best quality but costs ~$0.002 per request\n"
     ]
    }
   ],
   "source": [
    "# Uncomment if you have OpenAI API key\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "   temperature=0  # 0 = focused, factual responses\n",
    ")\n",
    "print(\"âœ“ Using OpenAI GPT-4o-mini\")\n",
    "\n",
    "print(\"ðŸ’¡ OpenAI provides best quality but costs ~$0.002 per request\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option B: HuggingFace (Free, Local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Using HuggingFace: Mistral-7B-Instruct\n",
      "  - Free to use (rate limited)\n",
      "  - Good quality for most tasks\n"
     ]
    },
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mStopIteration\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m  - Good quality for most tasks\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Test the LLM\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m test_response = \u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mWhat is 2+2? Answer in one word.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mTest: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_response\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Langchain Training\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:373\u001b[39m, in \u001b[36mBaseLLM.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    362\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    363\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    364\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    369\u001b[39m     **kwargs: Any,\n\u001b[32m    370\u001b[39m ) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m    371\u001b[39m     config = ensure_config(config)\n\u001b[32m    372\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m--> \u001b[39m\u001b[32m373\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    374\u001b[39m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    375\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    376\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    377\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    378\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    380\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    381\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    382\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    383\u001b[39m         .generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m]\n\u001b[32m    384\u001b[39m         .text\n\u001b[32m    385\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Langchain Training\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:784\u001b[39m, in \u001b[36mBaseLLM.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m    775\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    776\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m    777\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    781\u001b[39m     **kwargs: Any,\n\u001b[32m    782\u001b[39m ) -> LLMResult:\n\u001b[32m    783\u001b[39m     prompt_strings = [p.to_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m--> \u001b[39m\u001b[32m784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_strings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Langchain Training\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:1006\u001b[39m, in \u001b[36mBaseLLM.generate\u001b[39m\u001b[34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    987\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m get_llm_cache() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[32m    988\u001b[39m     run_managers = [\n\u001b[32m    989\u001b[39m         callback_manager.on_llm_start(\n\u001b[32m    990\u001b[39m             \u001b[38;5;28mself\u001b[39m._serialized,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1004\u001b[39m         )\n\u001b[32m   1005\u001b[39m     ]\n\u001b[32m-> \u001b[39m\u001b[32m1006\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1007\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1008\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1009\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1010\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnew_arg_supported\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnew_arg_supported\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1011\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1012\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1013\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) > \u001b[32m0\u001b[39m:\n\u001b[32m   1014\u001b[39m     run_managers = [\n\u001b[32m   1015\u001b[39m         callback_managers[idx].on_llm_start(\n\u001b[32m   1016\u001b[39m             \u001b[38;5;28mself\u001b[39m._serialized,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1023\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m missing_prompt_idxs\n\u001b[32m   1024\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Langchain Training\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:810\u001b[39m, in \u001b[36mBaseLLM._generate_helper\u001b[39m\u001b[34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[39m\n\u001b[32m    799\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_generate_helper\u001b[39m(\n\u001b[32m    800\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    801\u001b[39m     prompts: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m    806\u001b[39m     **kwargs: Any,\n\u001b[32m    807\u001b[39m ) -> LLMResult:\n\u001b[32m    808\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    809\u001b[39m         output = (\n\u001b[32m--> \u001b[39m\u001b[32m810\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    811\u001b[39m \u001b[43m                \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    812\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    813\u001b[39m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# TODO: support multiple run managers\u001b[39;49;00m\n\u001b[32m    814\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    815\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    816\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    817\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[32m    818\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._generate(prompts, stop=stop)\n\u001b[32m    819\u001b[39m         )\n\u001b[32m    820\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    821\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Langchain Training\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:1500\u001b[39m, in \u001b[36mLLM._generate\u001b[39m\u001b[34m(self, prompts, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1497\u001b[39m new_arg_supported = inspect.signature(\u001b[38;5;28mself\u001b[39m._call).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1498\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[32m   1499\u001b[39m     text = (\n\u001b[32m-> \u001b[39m\u001b[32m1500\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1501\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[32m   1502\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call(prompt, stop=stop, **kwargs)\n\u001b[32m   1503\u001b[39m     )\n\u001b[32m   1504\u001b[39m     generations.append([Generation(text=text)])\n\u001b[32m   1505\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m LLMResult(generations=generations)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Langchain Training\\.venv\\Lib\\site-packages\\langchain_huggingface\\llms\\huggingface_endpoint.py:317\u001b[39m, in \u001b[36mHuggingFaceEndpoint._call\u001b[39m\u001b[34m(self, prompt, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m    314\u001b[39m         completion += chunk.text\n\u001b[32m    315\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m completion\n\u001b[32m--> \u001b[39m\u001b[32m317\u001b[39m response_text = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtext_generation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    318\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    319\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    320\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minvocation_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    321\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    323\u001b[39m \u001b[38;5;66;03m# Maybe the generation has stopped at one of the stop sequences:\u001b[39;00m\n\u001b[32m    324\u001b[39m \u001b[38;5;66;03m# then we remove this stop sequence from the end of the generated text\u001b[39;00m\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m stop_seq \u001b[38;5;129;01min\u001b[39;00m invocation_params[\u001b[33m\"\u001b[39m\u001b[33mstop\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Langchain Training\\.venv\\Lib\\site-packages\\huggingface_hub\\inference\\_client.py:2356\u001b[39m, in \u001b[36mInferenceClient.text_generation\u001b[39m\u001b[34m(self, prompt, details, stream, model, adapter_id, best_of, decoder_input_details, do_sample, frequency_penalty, grammar, max_new_tokens, repetition_penalty, return_full_text, seed, stop, stop_sequences, temperature, top_k, top_n_tokens, top_p, truncate, typical_p, watermark)\u001b[39m\n\u001b[32m   2350\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   2351\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mAPI endpoint/model for text-generation is not served via TGI. Cannot return output as a stream.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2352\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m Please pass `stream=False` as input.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2353\u001b[39m         )\n\u001b[32m   2355\u001b[39m model_id = model \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model\n\u001b[32m-> \u001b[39m\u001b[32m2356\u001b[39m provider_helper = \u001b[43mget_provider_helper\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprovider\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtext-generation\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2357\u001b[39m request_parameters = provider_helper.prepare_request(\n\u001b[32m   2358\u001b[39m     inputs=prompt,\n\u001b[32m   2359\u001b[39m     parameters=parameters,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2363\u001b[39m     api_key=\u001b[38;5;28mself\u001b[39m.token,\n\u001b[32m   2364\u001b[39m )\n\u001b[32m   2366\u001b[39m \u001b[38;5;66;03m# Handle errors separately for more precise error messages\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Langchain Training\\.venv\\Lib\\site-packages\\huggingface_hub\\inference\\_providers\\__init__.py:217\u001b[39m, in \u001b[36mget_provider_helper\u001b[39m\u001b[34m(provider, task, model)\u001b[39m\n\u001b[32m    215\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mSpecifying a model is required when provider is \u001b[39m\u001b[33m'\u001b[39m\u001b[33mauto\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    216\u001b[39m     provider_mapping = _fetch_inference_provider_mapping(model)\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     provider = \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(provider_mapping)).provider\n\u001b[32m    219\u001b[39m provider_tasks = PROVIDERS.get(provider)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m    220\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m provider_tasks \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mStopIteration\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "# Use free HuggingFace Inference API\n",
    "llm = HuggingFaceEndpoint(\n",
    "    #repo_id=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    #repo_id=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    repo_id=\"HuggingFaceH4/zephyr-7b-beta\",\n",
    "    temperature=0.1,\n",
    "    max_new_tokens=512,\n",
    ")\n",
    "\n",
    "print(\"âœ“ Using HuggingFace: Mistral-7B-Instruct\")\n",
    "print(\"  - Free to use (rate limited)\")\n",
    "print(\"  - Good quality for most tasks\")\n",
    "\n",
    "# Test the LLM\n",
    "test_response = llm.invoke(\"What is 2+2? Answer in one word.\")\n",
    "print(f\"\\nTest: {test_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ðŸ’¡ Note:** HuggingFace free tier has rate limits. For production, use OpenAI or host your own model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Step 7: Build the RAG Chain ðŸ”—\n",
    "\n",
    "This is where the magic happens! Connect retriever + LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ RAG Chain created successfully!\n",
      "  - Retriever: Finds relevant chunks\n",
      "  - LLM: Generates answer from chunks\n",
      "  - Returns: Answer + source documents\n"
     ]
    }
   ],
   "source": [
    "from langchain_classic.chains import RetrievalQA \n",
    "#from langchain_classic.chains import create_retrieval_chain\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Create a custom prompt\n",
    "prompt_template = \"\"\"Use the following pieces of context to answer the question at the end. \n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer: Let me help you with that.\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, \n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "# Create the RAG chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=model,\n",
    "    chain_type=\"stuff\",  # \"stuff\" = put all retrieved docs into prompt\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,  # Return sources with answer\n",
    "    chain_type_kwargs={\"prompt\": PROMPT}\n",
    ")\n",
    "\n",
    "print(\"âœ“ RAG Chain created successfully!\")\n",
    "print(\"  - Retriever: Finds relevant chunks\")\n",
    "print(\"  - LLM: Generates answer from chunks\")\n",
    "print(\"  - Returns: Answer + source documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.middleware import dynamic_prompt, ModelRequest\n",
    "\n",
    "@dynamic_prompt\n",
    "def prompt_with_context(request: ModelRequest) -> str:\n",
    "    \"\"\"Inject context into state messages.\"\"\"\n",
    "    last_query = request.state[\"messages\"][-1].text\n",
    "    retrieved_docs = vectorstore.similarity_search(last_query)\n",
    "\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n",
    "\n",
    "    system_message = (\n",
    "        \"You are a helpful assistant. Use the following context in your response:\"\n",
    "        f\"\\n\\n{docs_content}\"\n",
    "    )\n",
    "\n",
    "    return system_message\n",
    "\n",
    "\n",
    "agent = create_agent(model, tools=[], middleware=[prompt_with_context])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What are vector embeddings?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Vector embeddings are numerical representations of text that capture the semantic meaning of the content. They transform words, phrases, or entire documents into fixed-size vectors of numbers, allowing for the comparison of their meanings based on their proximity in the vector space. Similar texts produce similar embedding vectors, which makes it easier to perform tasks like similarity search, clustering, and classification.\n",
      "\n",
      "Common embedding models include OpenAI's text-embedding-3-small, sentence-transformers from HuggingFace like all-MiniLM-L6-v2, and Google's text-embedding-004. These embeddings typically have dimensions ranging from 384 to 1536, depending on the model used. By representing text in this way, vector embeddings enable various applications in natural language processing and machine learning.\n"
     ]
    }
   ],
   "source": [
    "query = \"What are vector embeddings?\"\n",
    "for step in agent.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    step[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ðŸ’¡ What's happening in the chain:**\n",
    "1. User asks question\n",
    "2. Retriever finds 3 most relevant chunks\n",
    "3. Chunks inserted into prompt as \"context\"\n",
    "4. LLM generates answer using that context\n",
    "5. Sources returned for verification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Step 8: Test Your RAG ChatBot! ðŸŽ‰\n",
    "\n",
    "Let's ask some questions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Question: What is LangChain?\n",
      "======================================================================\n",
      "\n",
      "Answer:\n",
      "LangChain is a framework for developing applications powered by language models. It enables applications that are context-aware and can reason about information. LangChain provides tools for document loading, text splitting, embeddings, vector stores, and chains for building complex LLM applications.\n",
      "\n",
      "======================================================================\n",
      "Sources:\n",
      "\n",
      "1. langchain_intro.txt\n",
      "   LangChain is a framework for developing applications powered by language models. \n",
      "        It enables applications that are context-aware and can reaso...\n",
      "\n",
      "2. chroma_db.txt\n",
      "   Chroma is an open-source embedding database designed for AI applications. \n",
      "        It provides fast similarity search, easy integration with LangChain...\n",
      "\n",
      "3. splitting_guide.txt\n",
      "   Document splitting is crucial for RAG systems. The RecursiveCharacterTextSplitter \n",
      "        is recommended as it intelligently splits on paragraphs, se...\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "def ask_question(question):\n",
    "    \"\"\"Ask a question and display answer with sources.\"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    result = qa_chain.invoke({\"query\": question})\n",
    "    \n",
    "    print(f\"Answer:\\n{result['result']}\\n\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Sources:\")\n",
    "    for i, doc in enumerate(result['source_documents'], 1):\n",
    "        print(f\"\\n{i}. {doc.metadata.get('source', 'Unknown')}\")\n",
    "        print(f\"   {doc.page_content[:150]}...\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "# Test with sample questions\n",
    "questions = [\n",
    "    \"What is LangChain?\",\n",
    "    \"How does RAG work?\",\n",
    "    \"What embedding models are commonly used?\",\n",
    "    \"Why is document splitting important?\"\n",
    "]\n",
    "\n",
    "# Ask the first question\n",
    "ask_question(questions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try more questions\n",
    "ask_question(questions[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Question: What is Chroma and why use it?\n",
      "======================================================================\n",
      "\n",
      "Answer:\n",
      "Chroma is an open-source embedding database designed for AI applications. It provides fast similarity search, easy integration with LangChain, and persistent storage. Chroma supports metadata filtering and is particularly well-suited for building Retrieval-Augmented Generation (RAG) applications. It can be run locally or in a client-server mode, making it versatile for different deployment needs. Using Chroma allows developers to efficiently manage and retrieve embeddings, enhancing the performance of AI models that rely on semantic similarity.\n",
      "\n",
      "======================================================================\n",
      "Sources:\n",
      "\n",
      "1. chroma_db.txt\n",
      "   Chroma is an open-source embedding database designed for AI applications. \n",
      "        It provides fast similarity search, easy integration with LangChain...\n",
      "\n",
      "2. splitting_guide.txt\n",
      "   Document splitting is crucial for RAG systems. The RecursiveCharacterTextSplitter \n",
      "        is recommended as it intelligently splits on paragraphs, se...\n",
      "\n",
      "3. embeddings_guide.txt\n",
      "   Vector embeddings are numerical representations of text that capture semantic meaning. \n",
      "        Similar texts produce similar embedding vectors. Commo...\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Ask your own question!\n",
    "custom_question = \"What is Chroma and why use it?\"  # Change this!\n",
    "ask_question(custom_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Question: Who is Donald Trump?\n",
      "======================================================================\n",
      "\n",
      "Answer:\n",
      "I don't know.\n",
      "\n",
      "======================================================================\n",
      "Sources:\n",
      "\n",
      "1. chroma_db.txt\n",
      "   Chroma is an open-source embedding database designed for AI applications. \n",
      "        It provides fast similarity search, easy integration with LangChain...\n",
      "\n",
      "2. embeddings_guide.txt\n",
      "   Vector embeddings are numerical representations of text that capture semantic meaning. \n",
      "        Similar texts produce similar embedding vectors. Commo...\n",
      "\n",
      "3. splitting_guide.txt\n",
      "   Document splitting is crucial for RAG systems. The RecursiveCharacterTextSplitter \n",
      "        is recommended as it intelligently splits on paragraphs, se...\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Ask your own question!\n",
    "custom_question = \"Who is Donald Trump?\"  # Change this!\n",
    "ask_question(custom_question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ðŸ’¡ Notice:**\n",
    "- âœ… Answers are grounded in your documents\n",
    "- âœ… Sources are provided for verification\n",
    "- âœ… No hallucination (answers \"I don't know\" when info not in docs)\n",
    "\n",
    "**This is the power of RAG!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Step 9: Interactive Chat Interface ðŸ’¬\n",
    "\n",
    "Let's create a simple chat loop for continuous interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "RAG ChatBot Ready! Ask me anything about the documents.\n",
      "Type 'quit' or 'exit' to end the conversation.\n",
      "======================================================================\n",
      "\n",
      "\n",
      "Bot: Vector embeddings are numerical representations of text that capture semantic meaning. They are designed such that similar texts produce similar embedding vectors. Common embedding models include OpenAI's text-embedding-3-small, sentence-transformers from HuggingFace like all-MiniLM-L6-v2, and Google's text-embedding-004. Embeddings typically have dimensions ranging from 384 to 1536.\n",
      "\n",
      "ðŸ“š Sources: embeddings_guide.txt, chroma_db.txt, splitting_guide.txt\n",
      "\n",
      "Bot: I don't know.\n",
      "\n",
      "ðŸ“š Sources: chroma_db.txt, embeddings_guide.txt, splitting_guide.txt\n",
      "\n",
      "Bot: I don't know.\n",
      "\n",
      "ðŸ“š Sources: chroma_db.txt, splitting_guide.txt, embeddings_guide.txt\n",
      "\n",
      "Goodbye! ðŸ‘‹\n",
      "ðŸ’¡ Uncomment 'chat_loop()' above to start interactive chat!\n"
     ]
    }
   ],
   "source": [
    "def chat_loop():\n",
    "    \"\"\"\n",
    "    Simple chat interface.\n",
    "    Type 'quit' or 'exit' to end.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"RAG ChatBot Ready! Ask me anything about the documents.\")\n",
    "    print(\"Type 'quit' or 'exit' to end the conversation.\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    while True:\n",
    "        # Get user input\n",
    "        question = input(\"\\nYou: \").strip()\n",
    "        \n",
    "        # Check for exit\n",
    "        if question.lower() in ['quit', 'exit', 'q']:\n",
    "            print(\"\\nGoodbye! ðŸ‘‹\")\n",
    "            break\n",
    "        \n",
    "        if not question:\n",
    "            continue\n",
    "        \n",
    "        # Get answer\n",
    "        try:\n",
    "            result = qa_chain.invoke({\"query\": question})\n",
    "            print(f\"\\nBot: {result['result']}\")\n",
    "            print(f\"\\nðŸ“š Sources: {', '.join([doc.metadata.get('source', 'N/A') for doc in result['source_documents']])}\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError: {e}\")\n",
    "            print(\"Please try again.\")\n",
    "\n",
    "# Uncomment to start chat\n",
    "chat_loop()\n",
    "\n",
    "print(\"ðŸ’¡ Uncomment 'chat_loop()' above to start interactive chat!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Bonus: Simple Streamlit UI ðŸŽ¨\n",
    "\n",
    "Want a web interface? Create `app.py` with this code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "streamlit_code = '''\n",
    "import streamlit as st\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings, HuggingFaceEndpoint\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Page config\n",
    "st.set_page_config(page_title=\"RAG ChatBot\", page_icon=\"ðŸ¤–\")\n",
    "st.title(\"ðŸ¤– RAG ChatBot\")\n",
    "st.caption(\"Ask questions about your documents!\")\n",
    "\n",
    "# Load components (cached)\n",
    "@st.cache_resource\n",
    "def load_rag_chain():\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "    vectorstore = Chroma(\n",
    "        persist_directory=\"./chroma_rag_db\",\n",
    "        embedding_function=embeddings,\n",
    "        collection_name=\"rag_demo\"\n",
    "    )\n",
    "    retriever = vectorstore.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 3})\n",
    "    llm = HuggingFaceEndpoint(repo_id=\"mistralai/Mistral-7B-Instruct-v0.2\", temperature=0.1)\n",
    "    \n",
    "    return RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        retriever=retriever,\n",
    "        return_source_documents=True\n",
    "    )\n",
    "\n",
    "qa_chain = load_rag_chain()\n",
    "\n",
    "# Chat interface\n",
    "if \"messages\" not in st.session_state:\n",
    "    st.session_state.messages = []\n",
    "\n",
    "# Display chat history\n",
    "for message in st.session_state.messages:\n",
    "    with st.chat_message(message[\"role\"]):\n",
    "        st.write(message[\"content\"])\n",
    "\n",
    "# User input\n",
    "if prompt := st.chat_input(\"Ask a question...\"):\n",
    "    # Add user message\n",
    "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "    with st.chat_message(\"user\"):\n",
    "        st.write(prompt)\n",
    "    \n",
    "    # Get response\n",
    "    with st.chat_message(\"assistant\"):\n",
    "        with st.spinner(\"Thinking...\"):\n",
    "            result = qa_chain.invoke({\"query\": prompt})\n",
    "            response = result[\"result\"]\n",
    "            sources = [doc.metadata.get(\"source\", \"N/A\") for doc in result[\"source_documents\"]]\n",
    "            \n",
    "            st.write(response)\n",
    "            st.caption(f\"ðŸ“š Sources: {', '.join(set(sources))}\")\n",
    "    \n",
    "    # Add assistant message\n",
    "    st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "'''\n",
    "\n",
    "# Save to file\n",
    "with open('streamlit_app.py', 'w') as f:\n",
    "    f.write(streamlit_code)\n",
    "\n",
    "print(\"âœ“ Created streamlit_app.py\")\n",
    "print(\"\\nTo run:\")\n",
    "print(\"  pip install streamlit\")\n",
    "print(\"  streamlit run streamlit_app.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# What You've Built! ðŸŽ‰\n",
    "\n",
    "**Congratulations!** You now have a fully functional RAG chatbot that:\n",
    "\n",
    "âœ… **Loads documents** from multiple sources (PDF, web, text)\n",
    "âœ… **Splits intelligently** using RecursiveCharacterTextSplitter\n",
    "âœ… **Creates embeddings** using HuggingFace (free!)\n",
    "âœ… **Stores in Chroma** with persistent storage\n",
    "âœ… **Retrieves with MMR** for diverse, relevant results\n",
    "âœ… **Generates answers** using LLM (Mistral or GPT)\n",
    "âœ… **Shows sources** for verification\n",
    "âœ… **Has a UI** (Streamlit)\n",
    "\n",
    "**All in ~30 minutes!**\n",
    "\n",
    "---\n",
    "\n",
    "## Performance & Cost\n",
    "\n",
    "**With Free Options (HuggingFace):**\n",
    "- ðŸ’° Cost: $0\n",
    "- âš¡ Speed: 2-5 seconds per query\n",
    "- ðŸ“Š Quality: Good for most use cases\n",
    "\n",
    "**With OpenAI:**\n",
    "- ðŸ’° Cost: ~$0.002 per query\n",
    "- âš¡ Speed: <1 second per query\n",
    "- ðŸ“Š Quality: Excellent\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps ðŸš€\n",
    "\n",
    "**You can now:**\n",
    "1. Replace sample docs with YOUR documents\n",
    "2. Adjust chunk size/overlap for your use case\n",
    "3. Try different embedding models\n",
    "4. Experiment with retrieval parameters\n",
    "5. Customize the prompt for your domain\n",
    "\n",
    "**In Part 5b (Coming Next):**\n",
    "- ðŸ”¥ Conversational RAG (chat history)\n",
    "- ðŸ”¥ Advanced retrieval (re-ranking, hybrid search)\n",
    "- ðŸ”¥ Evaluation metrics (measure quality)\n",
    "- ðŸ”¥ Production optimization (caching, streaming)\n",
    "- ðŸ”¥ Error handling & edge cases\n",
    "\n",
    "---\n",
    "\n",
    "## Common Issues & Solutions\n",
    "\n",
    "**Problem: \"Rate limit exceeded\" with HuggingFace**\n",
    "- Solution: Use OpenAI or host model locally with Ollama\n",
    "\n",
    "**Problem: \"Slow responses\"**\n",
    "- Solution: Use GPU for embeddings, try smaller chunks\n",
    "\n",
    "**Problem: \"Answers not accurate\"**\n",
    "- Solution: Adjust chunk size, try k=5 instead of k=3, improve prompts\n",
    "\n",
    "**Problem: \"Can't find answer in documents\"**\n",
    "- Solution: Check if info is actually in docs, try similarity search directly\n",
    "\n",
    "---\n",
    "\n",
    "## Resources\n",
    "\n",
    "- [LangChain Documentation](https://python.langchain.com/)\n",
    "- [Chroma Documentation](https://docs.trychroma.com/)\n",
    "- [HuggingFace Models](https://huggingface.co/models)\n",
    "- [My Previous Blogs](#) - Parts 1-4 of this series\n",
    "\n",
    "---\n",
    "\n",
    "# Thank You! ðŸ™\n",
    "\n",
    "**You've completed the journey:**\n",
    "1. âœ… Document Loading\n",
    "2. âœ… Document Splitting\n",
    "3. âœ… Vector Embeddings\n",
    "4. âœ… Vector Databases\n",
    "5. âœ… **Building RAG ChatBot** â† You are here!\n",
    "\n",
    "**What's next?** Try it with YOUR documents and share your results!\n",
    "\n",
    "**Questions?** Drop them in the comments below!\n",
    "\n",
    "**Stay tuned for Part 5b** - Production RAG patterns! ðŸš€\n",
    "\n",
    "#LangChain #RAG #AI #MachineLearning #Python #ChatBot #VectorDatabase #LLM"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-training",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
