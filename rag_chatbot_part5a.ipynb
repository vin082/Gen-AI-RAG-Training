{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Your First RAG ChatBot with LangChain\n",
    "## Part 5a: From Documents to Intelligent Q&A in 30 Minutes\n",
    "\n",
    "**Welcome to the finale of our LangChain RAG series!** üéâ\n",
    "\n",
    "In the previous articles, we learned:\n",
    "- **Part 1**: [Document Loading](link) - Getting data into your system\n",
    "- **Part 2**: [Document Splitting](blog_document_splitting.md) - Optimal chunking strategies\n",
    "- **Part 3**: [Vector Embeddings](blog_vector_embeddings.md) - Semantic understanding\n",
    "- **Part 4**: [Vector Databases](link) - Storage and retrieval at scale\n",
    "\n",
    "**Now we bring it ALL together!** üöÄ\n",
    "\n",
    "In this notebook, you'll build a working RAG chatbot that:\n",
    "- ‚úÖ Answers questions about YOUR documents\n",
    "- ‚úÖ Shows sources for answers\n",
    "- ‚úÖ Runs completely locally (free!)\n",
    "- ‚úÖ Takes ~30 minutes to set up\n",
    "\n",
    "---\n",
    "\n",
    "## What is RAG Again?\n",
    "\n",
    "**RAG (Retrieval Augmented Generation)** = Smart Q&A over your documents\n",
    "\n",
    "**The Problem:**\n",
    "- LLMs don't know about YOUR specific documents\n",
    "- They can't access real-time or proprietary information\n",
    "- They \"hallucinate\" when they don't know answers\n",
    "\n",
    "**The Solution (RAG):**\n",
    "1. User asks a question\n",
    "2. System retrieves relevant document chunks\n",
    "3. LLM generates answer using those chunks\n",
    "4. Result: Accurate, grounded responses with sources!\n",
    "\n",
    "---\n",
    "\n",
    "**üí° By the end:** You'll have a chatbot answering questions about any PDF, website, or text you provide!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture Overview\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  Documents  ‚îÇ (PDFs, websites, text)\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "       ‚îÇ\n",
    "       ‚ñº\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ   Loader    ‚îÇ (Read documents)\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "       ‚îÇ\n",
    "       ‚ñº\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  Splitter   ‚îÇ (Chunk into pieces)\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "       ‚îÇ\n",
    "       ‚ñº\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ Embeddings  ‚îÇ (Convert to vectors)\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "       ‚îÇ\n",
    "       ‚ñº\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ Vector DB   ‚îÇ (Store in Chroma)\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "       ‚îÇ\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ             ‚îÇ\n",
    "‚ñº             ‚ñº\n",
    "User ‚îÄ‚îÄ‚ñ∫ Retriever ‚îÄ‚îÄ‚ñ∫ LLM ‚îÄ‚îÄ‚ñ∫ Answer\n",
    "Question     (Find       (Generate\n",
    "             relevant)   response)\n",
    "```\n",
    "\n",
    "**Simple, right?** Let's build it step by step!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install all required packages\n",
    "pip install -q langchain langchain-community langchain-core langchain-chroma\n",
    "pip install -q langchain-huggingface sentence-transformers\n",
    "pip install -q langchain-openai langchain-google-genai google-generativeai langchain-groq langchain-huggingface langchain-tavily\n",
    "pip install -q pypdf pymupdf  # For PDF loading\n",
    "pip install -q chromadb faiss-cpu # Vector database\n",
    "\n",
    "print(\"‚úì All packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: API Keys Setup\n",
    "\n",
    "**Two options for LLM:**\n",
    "1. **OpenAI** (paid, best quality) - Requires API key\n",
    "2. **Local LLM** (free, good enough) - No API key needed\n",
    "\n",
    "We'll show both approaches!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Configuration ready\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Option 1: Use OpenAI (uncomment if you have API key)\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"GOOGLE_API_KEY\"] = os.getenv(\"GOOGLE_API_KEY\")\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "# Option 2: We'll use HuggingFace (free, local)\n",
    "# No API key needed!\n",
    "\n",
    "print(\"‚úì Configuration ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#For Google Colab\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgoogle\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcolab\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m userdata\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Try to load from Colab Secrets first\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "#For Google Colab\n",
    "\n",
    "import os\n",
    "from google.colab import userdata\n",
    "\n",
    "\n",
    "OPENAI_API_KEY=userdata.get('OPENAI_API_KEY')\n",
    "GEMINI_API_KEY=userdata.get('GEMINI_API_KEY')\n",
    "GROQ_API_KEY=userdata.get('GROQ_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Step 1: Load Documents üìÑ\n",
    "\n",
    "First, we need documents to answer questions about. We'll use multiple sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option A: Load from Sample Text (Quick Start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Loaded 5 sample documents\n",
      "\n",
      "Sample content: LangChain is a framework for developing applications powered by language models. \n",
      "        It enables...\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "# Sample documents about LangChain and RAG\n",
    "sample_docs = [\n",
    "    Document(\n",
    "        page_content=\"\"\"LangChain is a framework for developing applications powered by language models. \n",
    "        It enables applications that are context-aware and can reason about information. LangChain provides \n",
    "        tools for document loading, text splitting, embeddings, vector stores, and chains for building \n",
    "        complex LLM applications.\"\"\",\n",
    "        metadata={\"source\": \"langchain_intro.txt\", \"topic\": \"LangChain Overview\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"\"\"Retrieval Augmented Generation (RAG) is a technique that combines information retrieval \n",
    "        with text generation. It works by first retrieving relevant documents from a knowledge base, then \n",
    "        passing those documents to a language model to generate accurate, grounded responses. This prevents \n",
    "        hallucinations and allows LLMs to answer questions about specific, up-to-date information.\"\"\",\n",
    "        metadata={\"source\": \"rag_explanation.txt\", \"topic\": \"RAG\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"\"\"Vector embeddings are numerical representations of text that capture semantic meaning. \n",
    "        Similar texts produce similar embedding vectors. Common embedding models include OpenAI's text-embedding-3-small, \n",
    "        sentence-transformers from HuggingFace like all-MiniLM-L6-v2, and Google's text-embedding-004. \n",
    "        Embeddings typically have 384 to 1536 dimensions.\"\"\",\n",
    "        metadata={\"source\": \"embeddings_guide.txt\", \"topic\": \"Embeddings\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"\"\"Chroma is an open-source embedding database designed for AI applications. \n",
    "        It provides fast similarity search, easy integration with LangChain, and persistent storage. \n",
    "        Chroma supports metadata filtering and is perfect for building RAG applications. It can run \n",
    "        locally or in client-server mode.\"\"\",\n",
    "        metadata={\"source\": \"chroma_db.txt\", \"topic\": \"Vector Databases\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"\"\"Document splitting is crucial for RAG systems. The RecursiveCharacterTextSplitter \n",
    "        is recommended as it intelligently splits on paragraphs, sentences, and words in order. \n",
    "        Optimal chunk size is typically 500-1000 characters with 10-20% overlap. The overlap ensures \n",
    "        context isn't lost at chunk boundaries.\"\"\",\n",
    "        metadata={\"source\": \"splitting_guide.txt\", \"topic\": \"Document Splitting\"}\n",
    "    ),\n",
    "]\n",
    "\n",
    "print(f\"‚úì Loaded {len(sample_docs)} sample documents\")\n",
    "print(f\"\\nSample content: {sample_docs[0].page_content[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option B: Load from PDF (Your Own Documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\RAG WorkingSession\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Loaded 93 pages from PDF\n"
     ]
    }
   ],
   "source": [
    "# Uncomment to load your own PDF\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "\n",
    "loader = PyMuPDFLoader(\"data/Mastering AI Agents.pdf\")\n",
    "sample_docs = loader.load()\n",
    "print(f\"‚úì Loaded {len(sample_docs)} pages from PDF\")\n",
    "\n",
    "#print(\"üí° To use your own PDF: uncomment above and provide PDF path\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option C: Load from Website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to load from a website\n",
    "# from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "# loader = WebBaseLoader(\"https://python.langchain.com/docs/get_started/introduction\")\n",
    "# sample_docs = loader.load()\n",
    "# print(f\"‚úì Loaded content from website\")\n",
    "\n",
    "print(\"üí° To load from web: uncomment above and provide URL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Step 2: Split Documents into Chunks üî™\n",
    "\n",
    "We split documents into smaller chunks for better retrieval precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Split 5 documents into 5 chunks\n",
      "\n",
      "Sample chunk:\n",
      "Content: LangChain is a framework for developing applications powered by language models. \n",
      "        It enables applications that are context-aware and can reason about information. LangChain provides \n",
      "        tools for document loading, text splitting, embeddings, vector stores, and chains for building \n",
      "        complex LLM applications.\n",
      "Metadata: {'source': 'langchain_intro.txt', 'topic': 'LangChain Overview'}\n",
      "Chunk size: 328 characters\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Create text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,        # Size of each chunk\n",
    "    chunk_overlap=50,      # Overlap between chunks (preserves context)\n",
    "    length_function=len,   # How to measure chunk size\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]  # Split on paragraphs, then sentences, then words\n",
    ")\n",
    "\n",
    "# Split the documents\n",
    "splits = text_splitter.split_documents(sample_docs)\n",
    "\n",
    "print(f\"‚úì Split {len(sample_docs)} documents into {len(splits)} chunks\")\n",
    "print(f\"\\nSample chunk:\")\n",
    "print(f\"Content: {splits[0].page_content}\")\n",
    "print(f\"Metadata: {splits[0].metadata}\")\n",
    "print(f\"Chunk size: {len(splits[0].page_content)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üí° Why splitting matters:**\n",
    "- Too large: Retrieves irrelevant info, wastes LLM context\n",
    "- Too small: Loses context, poor answers\n",
    "- Just right: Precise retrieval, better answers!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Step 3: Create Embeddings üßÆ\n",
    "\n",
    "Convert text chunks into vector embeddings for semantic search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LRScheduler' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_huggingface\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HuggingFaceEmbeddings\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Initialize embeddings model (free, runs locally)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m embeddings = \u001b[43mHuggingFaceEmbeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mall-MiniLM-L6-v2\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Fast, 384 dimensions\u001b[39;49;00m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#model_name=\"all-mpnet-base-v2\",\u001b[39;49;00m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdevice\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcpu\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Use 'cuda' if you have GPU\u001b[39;49;00m\n\u001b[32m      8\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Test the embeddings\u001b[39;00m\n\u001b[32m     11\u001b[39m test_text = \u001b[33m\"\u001b[39m\u001b[33mWhat is LangChain?\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\RAG WorkingSession\\.venv\\Lib\\site-packages\\langchain_huggingface\\embeddings\\huggingface.py:69\u001b[39m, in \u001b[36mHuggingFaceEmbeddings.__init__\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m     67\u001b[39m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(**kwargs)\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[import]\u001b[39;00m\n\u001b[32m     70\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     71\u001b[39m     msg = (\n\u001b[32m     72\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCould not import sentence_transformers python package. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     73\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPlease install it with `pip install sentence-transformers`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     74\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\RAG WorkingSession\\.venv\\Lib\\site-packages\\sentence_transformers\\__init__.py:15\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwarnings\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackend\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     11\u001b[39m     export_dynamic_quantized_onnx_model,\n\u001b[32m     12\u001b[39m     export_optimized_onnx_model,\n\u001b[32m     13\u001b[39m     export_static_quantized_openvino_model,\n\u001b[32m     14\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcross_encoder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     16\u001b[39m     CrossEncoder,\n\u001b[32m     17\u001b[39m     CrossEncoderModelCardData,\n\u001b[32m     18\u001b[39m     CrossEncoderTrainer,\n\u001b[32m     19\u001b[39m     CrossEncoderTrainingArguments,\n\u001b[32m     20\u001b[39m )\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ParallelSentencesDataset, SentencesDataset\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mLoggingHandler\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LoggingHandler\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\RAG WorkingSession\\.venv\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\__init__.py:5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mCrossEncoder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CrossEncoder\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_card\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CrossEncoderModelCardData\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtrainer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CrossEncoderTrainer\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtraining_args\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CrossEncoderTrainingArguments\n\u001b[32m      8\u001b[39m __all__ = [\n\u001b[32m      9\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mCrossEncoder\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     10\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mCrossEncoderTrainer\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     11\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mCrossEncoderTrainingArguments\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     12\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mCrossEncoderModelCardData\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     13\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\RAG WorkingSession\\.venv\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\trainer.py:22\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcross_encoder\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtraining_args\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CrossEncoderTrainingArguments\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mevaluation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SentenceEvaluator, SequentialEvaluator\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtrainer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformerTrainer\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutil\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_datasets_available, is_training_available\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_datasets_available():\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\RAG WorkingSession\\.venv\\Lib\\site-packages\\sentence_transformers\\trainer.py:20\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata_collator\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataCollator\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mintegrations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m WandbCallback\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtrainer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TRAINING_ARGS_NAME\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtrainer_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m EvalLoopOutput\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata_collator\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformerDataCollator\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\RAG WorkingSession\\.venv\\Lib\\site-packages\\transformers\\trainer.py:74\u001b[39m\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodeling_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PreTrainedModel, load_sharded_checkpoint, unwrap_model\n\u001b[32m     70\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mauto\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodeling_auto\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     71\u001b[39m     MODEL_FOR_CAUSAL_LM_MAPPING_NAMES,\n\u001b[32m     72\u001b[39m     MODEL_MAPPING_NAMES,\n\u001b[32m     73\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01moptimization\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Adafactor, get_scheduler\n\u001b[32m     75\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprocessing_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ProcessorMixin\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpytorch_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     77\u001b[39m     is_torch_greater_or_equal_than_2_3,\n\u001b[32m     78\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\RAG WorkingSession\\.venv\\Lib\\site-packages\\transformers\\optimization.py:25\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01moptim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Optimizer\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01moptim\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlr_scheduler\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LambdaLR, ReduceLROnPlateau\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtrainer_pt_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LayerWiseDummyOptimizer, LayerWiseDummyScheduler\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtrainer_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SchedulerType\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m logging\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\RAG WorkingSession\\.venv\\Lib\\site-packages\\transformers\\trainer_pt_utils.py:1364\u001b[39m\n\u001b[32m   1360\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, closure=\u001b[38;5;28;01mNone\u001b[39;00m) -> Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[32m   1361\u001b[39m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1364\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mLayerWiseDummyScheduler\u001b[39;00m(\u001b[43mLRScheduler\u001b[49m):\n\u001b[32m   1365\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1366\u001b[39m \u001b[33;03m    For Layer-wise optimizers such as GaLoRE optimizer, the optimization and scheduling step\u001b[39;00m\n\u001b[32m   1367\u001b[39m \u001b[33;03m    are already done through the post gradient hooks. Therefore\u001b[39;00m\n\u001b[32m   1368\u001b[39m \u001b[33;03m    the trick is to create a dummy scheduler that can take arbitrary\u001b[39;00m\n\u001b[32m   1369\u001b[39m \u001b[33;03m    args and kwargs and return a no-op during training.\u001b[39;00m\n\u001b[32m   1370\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   1372\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n",
      "\u001b[31mNameError\u001b[39m: name 'LRScheduler' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# Initialize embeddings model (free, runs locally)\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"all-MiniLM-L6-v2\",  # Fast, 384 dimensions\n",
    "    #model_name=\"all-mpnet-base-v2\",\n",
    "    model_kwargs={'device': 'cpu'}  # Use 'cuda' if you have GPU\n",
    ")\n",
    "\n",
    "# Test the embeddings\n",
    "test_text = \"What is LangChain?\"\n",
    "test_embedding = embeddings.embed_query(test_text)\n",
    "\n",
    "print(f\"‚úì Embeddings model loaded\")\n",
    "print(f\"Model: all-MiniLM-L6-v2\")\n",
    "\n",
    "print(f\"Embedding dimensions: {len(test_embedding)}\")\n",
    "print(f\"Sample values: {test_embedding[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Alternative: Use OpenAI Embeddings (if you have API key)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Using OpenAI embeddings\n",
      "üí° OpenAI embeddings are higher quality but cost ~$0.02 per 1M tokens\n"
     ]
    }
   ],
   "source": [
    "# Uncomment if using OpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "print(\"‚úì Using OpenAI embeddings\")\n",
    "\n",
    "print(\"üí° OpenAI embeddings are higher quality but cost ~$0.02 per 1M tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OPtion3: Use Google Gemini Embedding Model(Free)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.023838477209210396, -0.008524507284164429, 0.010140136815607548, -0.03635908290743828, 0.005881804041564465, 0.017632408067584038, 0.03440266102552414, -0.022488268092274666, -0.037677180022001266, 0.09012677520513535, -0.041247013956308365, -0.07383030652999878, -0.03170005604624748, 0.03297711908817291, -0.027617353945970535, -0.07830660790205002, -0.010694045573472977, 0.01068727858364582, -0.07397930324077606, 0.0229897852987051, 0.0039268983528018, -0.009637449868023396, 0.025337379425764084, -0.008505502715706825, 0.04152005538344383, -0.045486852526664734, -0.014454089105129242, 0.028958335518836975, -0.010045764967799187, 0.002268982119858265, -0.04019991308450699, 0.04625634476542473, -0.027185451239347458, -0.03542790934443474, -0.0420713797211647, -0.01675763726234436, -0.01896270364522934, 0.01892778091132641, 0.0601670928299427, -0.021555563434958458, -0.037875574082136154, 0.03040831722319126, -0.030295928940176964, 0.02151942439377308, -0.03385293111205101, 0.0009651881409808993, 0.02175157330930233, 0.06143931299448013, -0.00477525545284152, -0.008410046808421612, 0.024281147867441177, -0.024492105469107628, -0.06216185912489891, 0.02122807316482067, -0.010186814703047276, -0.019851159304380417, -0.04470299556851387, -0.03296839818358421, 0.05440853536128998, 0.05786937102675438, 0.04904612526297569, 0.019667329266667366, 0.010561289265751839, 0.0015619152691215277, 0.019797326996922493, -0.031612738966941833, -0.015151875093579292, 0.011674493551254272, 0.025843162089586258, 0.010630208998918533, -0.0884372666478157, 0.06692977994680405, -0.004352086689323187, 0.058419276028871536, -0.04495362564921379, -0.052199557423591614, 0.03787405788898468, -0.03179454058408737, 0.035656534135341644, 0.043368496000766754, -0.036452170461416245, 0.009941763244569302, 0.08246863633394241, 0.02776351012289524, 0.021787023171782494, 0.04968374967575073, -0.0035606285091489553, -0.01998402737081051, -0.0004098141798749566, -0.0323462150990963, -0.020104361698031425, 0.024182355031371117, -0.05229223147034645, 0.04041682928800583, 0.06688500195741653, -0.02457483671605587, -0.03742695972323418, -0.05421648174524307, 0.050854481756687164, -0.03895207867026329, 0.017725870013237, 0.046300046145915985, -0.03798653930425644, -0.01757696084678173, 0.022033723071217537, 0.05238213762640953, 0.0313589945435524, -0.07050264626741409, -0.049911487847566605, -0.013098716735839844, 0.031829968094825745, -0.0017068092711269855, 0.020637180656194687, -0.004395567812025547, 0.003742404282093048, -0.02110411413013935, -0.04711977019906044, 0.01829807460308075, -0.02360662817955017, 0.04813220724463463, -0.048946231603622437, 0.012467458844184875, -0.03389131650328636, 0.05945786088705063, 0.039843086153268814, 0.022501492872834206, -0.019875045865774155, -0.005107729695737362, 0.022370900958776474, -0.07940257340669632, 0.04871241748332977, -0.01201015617698431, 0.040348052978515625, 0.009932911954820156, -0.04747019335627556, 0.01500686351209879, 0.038536109030246735, 0.012330764904618263, -0.051001694053411484, -0.04728066921234131, -0.018742868676781654, -0.04091402888298035, -0.03466656059026718, 0.0510873943567276, 0.051038190722465515, -0.03578193858265877, -0.040036216378211975, -0.04280469939112663, 0.009430557489395142, 0.04037575051188469, 0.005177621729671955, -0.018175454810261726, 0.01649952121078968, -0.00480858888477087, -0.020965810865163803, 0.04329502955079079, 0.039095692336559296, 0.0044346218928694725, 0.03390596807003021, 0.006399441044777632, -0.04038820415735245, -0.0007441381458193064, -0.04375990480184555, 0.022692836821079254, -0.023536164313554764, 0.018433745950460434, 0.04003138840198517, -0.01294713281095028, -0.025929760187864304, -0.07226359099149704, -0.037021275609731674, -0.0028428537771105766, 0.0018800456309691072, -0.04908370599150658, -0.026462752372026443, -0.012156389653682709, 0.010613344609737396, -0.004182708449661732, -0.0272294320166111, -0.0418824702501297, 0.05396430939435959, -0.008819744922220707, -0.025326108559966087, -0.011470455676317215, -0.00208239140920341, 0.06089983507990837, -0.03474046289920807, 0.027563901618123055, 0.009677566587924957, -0.04733096808195114, -0.053437575697898865, -0.03362125903367996, 0.02882486581802368, 0.03641967102885246, 0.043657660484313965, -0.009327860549092293, -0.01759123057126999, -0.05468740686774254, -0.03471194580197334, -0.01288404781371355, 0.00878982525318861, -0.018051911145448685, 0.06923827528953552, 0.02693525329232216, -0.05009027570486069, -0.03136729076504707, -0.08512605726718903, 0.003122686641290784, 0.005450264550745487, 0.04446527734398842, -0.01945318654179573, -0.04455055296421051, -0.01746484637260437, -0.06463860720396042, -0.003223998239263892, -0.0498417429625988, 0.06485755741596222, -0.03490648418664932, 0.10823392868041992, -0.0601717084646225, 0.03050754964351654, 0.04012424126267433, 0.10562588274478912, 0.013487836346030235, 0.023923605680465698, -0.008364121429622173, 0.036003295332193375, 0.00979998055845499, -0.023123018443584442, -0.04091847687959671, 0.032851435244083405, -0.00898862723261118, -0.03885936737060547, 0.011043834500014782, 0.011460193432867527, -0.03818172961473465, -0.04645829647779465, 0.010217614471912384, -0.021908633410930634, -0.018757866695523262, -0.012387766502797604, -0.06838897615671158, -0.016727812588214874, 0.008112628944218159, -0.015748780220746994, 0.05784192681312561, 0.028232896700501442, 0.06230949983000755, -0.040838345885276794, -0.029877549037337303, -0.05416668951511383, 0.05520084872841835, -0.02766883186995983, -0.026948601007461548, -0.05221029743552208, -0.05666729062795639, -0.008734812028706074, -0.016565773636102676, 0.035082340240478516, -0.001639185706153512, 0.014084593392908573, 0.023287687450647354, -0.07414132356643677, -0.016944358125329018, -0.10681680589914322, 0.0051445565186440945, -0.04697204381227493, -0.05723487585783005, -0.016577204689383507, -0.004408694338053465, -0.023386940360069275, -0.074443519115448, -0.037745825946331024, 0.02484920620918274, -0.03575993329286575, 0.03516853600740433, -0.04276159405708313, 0.014672786928713322, 0.0019339785212650895, -0.038101676851511, -0.011586546897888184, 0.03645863011479378, -0.01899299956858158, -0.0023919339291751385, -0.013169203884899616, 0.03373105823993683, -0.017884010449051857, 0.022948045283555984, -0.005463841371238232, -0.018939873203635216, -0.009008179418742657, 0.0397748239338398, 0.02817552722990513, -0.051315825432538986, 0.03666127100586891, 0.02032066322863102, 0.02827169932425022, 0.024465713649988174, -0.012152732349932194, 0.027807999402284622, 0.019824299961328506, 0.044274140149354935, -0.054336752742528915, -0.014281700365245342, -0.018607035279273987, -0.02743513137102127, 0.0005671657854691148, -0.04715046286582947, -0.011591716669499874, -0.015080268494784832, -0.037356194108724594, 0.06938011199235916, 0.019746048375964165, -0.019585352391004562, -0.019823867827653885, -0.030424347147345543, -0.0982646718621254, -0.0114689189940691, 0.02635648287832737, -0.01244458556175232, 0.045045189559459686, 0.04251300171017647, -0.0437108539044857, 0.019599981606006622, 0.03155023232102394, -0.01203089114278555, 0.03567097336053848, -0.022417081519961357, 0.03547510504722595, 0.0391010157763958, 0.007190866861492395, 0.004590238444507122, -0.012825860641896725, -0.012839464470744133, 0.002338010584935546, 0.07193241268396378, -0.028626635670661926, 0.05191247910261154, 0.014636475592851639, 0.008648627437651157, -0.002507549710571766, -0.017895014956593513, -0.004072181414812803, 0.05827869847416878, -0.017422549426555634, -0.05387961491942406, 0.021290525794029236, 0.010559843853116035, 0.056952398270368576, -0.026561886072158813, 0.040323738008737564, 0.04961251839995384, -0.03785986080765724, -0.031910598278045654, -0.021451439708471298, -0.0056448280811309814, 0.023803137242794037, -0.03083615191280842, 0.031516969203948975, -0.016917137429118156, -0.0024233083240687847, 0.028241785243153572, -0.02271106280386448, -0.026950649917125702, -0.06700462847948074, -0.019744038581848145, -0.004965555854141712, -0.0011986902682110667, -0.002057310426607728, 0.005483019165694714, 0.034237682819366455, 0.04827578365802765, -0.04775209352374077, 0.04553781449794769, 0.02419867552816868, -0.010444399900734425, -0.05749743804335594, 0.06784678250551224, -0.0018636671593412757, 0.014576422981917858, 0.002341268351301551, -0.025436799973249435, -0.05359762907028198, 0.018206562846899033, -0.012592366896569729, 0.03244626894593239, -0.054214637726545334, 0.01309595350176096, -0.08155737817287445, 0.007958886213600636, 0.03322276100516319, 0.006474474910646677, -0.004240165464580059, 0.037126574665308, -0.07178133726119995, 0.028934909030795097, -0.023325856775045395, 0.010867372155189514, -0.003995939623564482, -0.05481860041618347, -0.0002921473642345518, -0.022397708147764206, 0.03750721365213394, -0.0737450122833252, -0.016728118062019348, -0.03276309370994568, 0.02333778515458107, 0.03204178437590599, -0.008150570094585419, -0.027420524507761, -0.013969461433589458, -0.03155408799648285, -0.03654923662543297, -0.03689619526267052, -0.07861427962779999, 0.02090832032263279, -0.05355526879429817, -0.03386341407895088, 0.040659211575984955, 0.0352257676422596, -0.0019926701206713915, -0.0715288296341896, 0.02915521338582039, 0.05729439854621887, 0.01864725537598133, -0.039119359105825424, 0.006041252985596657, -0.0362069346010685, 0.0026395153254270554, 0.0026647159829735756, 0.0038053214084357023, 0.06473209708929062, 0.011965427547693253, 0.07433513551950455, 0.01603788323700428, -0.01720617339015007, 0.018540354445576668, 0.023902494460344315, 0.048504170030355453, 0.032007668167352676, -0.009308104403316975, 0.057526905089616776, 0.0021549141965806484, 0.029055817052721977, -0.026782790198922157, 0.07032432407140732, 0.05730598792433739, 0.005596735514700413, 0.004466646816581488, -0.09612677991390228, -0.02002202346920967, -0.0642315000295639, -0.05483965203166008, 0.0026192248333245516, 0.015864115208387375, -0.0005888781743124127, -0.019846364855766296, -0.029440652579069138, -0.0018167149974033237, 0.02861940674483776, 0.026034660637378693, 0.010646199807524681, 0.052049506455659866, 0.016396405175328255, -0.007141813635826111, -0.027730071917176247, 0.020911017432808876, -0.02007254771888256, -0.04130919277667999, -0.023328503593802452, 0.08233426511287689, 0.04116455093026161, 0.027065712958574295, -0.025180185213685036, 0.008248264901340008, 0.04044627398252487, 0.030202914029359818, 0.026248442009091377, -0.06160159409046173, -0.0832536593079567, 0.005469421856105328, 0.009732275269925594, 0.025337781757116318, 0.013533097691833973, 0.02846946381032467, 0.029145147651433945, -0.05164283514022827, -0.010305378586053848, -0.0007351314416155219, 0.0014027381548658013, -0.022849632427096367, 0.028657102957367897, -0.013583322986960411, -0.034602899104356766, 0.0116398511454463, -0.011079760268330574, 0.03883952274918556, -0.00425774184986949, 0.009519415907561779, 0.014479474164545536, 0.05276813730597496, 0.010722944512963295, -0.027804220095276833, -0.047358687967061996, -0.05011738836765289, 0.02090909145772457, 0.00032098969677463174, -0.029393604025244713, 0.05344710126519203, 0.0038458663038909435, -0.0035200424026697874, -0.009895999915897846, 0.022582629695534706, 0.08868276327848434, 0.03397262096405029, 0.003985302988439798, -0.05458200350403786, -0.02294260635972023, -0.05106246843934059, -0.005296742543578148, 0.013168553821742535, -0.007823474705219269, 0.060337133705616, -0.0563264861702919, 0.036984436213970184, -0.0382009856402874, -0.017497530207037926, 0.06467597931623459, 0.033519040793180466, 0.025696655735373497, -0.06312695145606995, 0.04549810290336609, -0.010817629285156727, 0.03239358216524124, 0.02257956750690937, 0.031669821590185165, -0.04411611706018448, 0.039039768278598785, -0.02670632116496563, 0.01059576217085123, -0.00963582843542099, -0.033642254769802094, 0.03894811496138573, 0.061128880828619, -0.01733432710170746, 0.009660091251134872, -0.010020669549703598, -0.002187867648899555, -0.01436618622392416, 0.03553832694888115, 0.023245949298143387, -0.028525792062282562, -0.0712737888097763, -0.02168223075568676, -0.04395914077758789, -0.011909286491572857, 0.024781623855233192, 0.012490151450037956, -0.045449767261743546, 0.03231683000922203, -0.01408470794558525, -7.807990414221422e-07, 0.01985163800418377, -0.01565040647983551, -0.0182249303907156, 0.05159813165664673, -0.03707011789083481, 0.04667956009507179, -0.012622516602277756, 0.02033909596502781, -0.01638963259756565, 7.590308086946607e-05, -0.017259720712900162, -0.03557172417640686, 0.023278342559933662, 0.055861394852399826, -0.026633018627762794, 0.025482704862952232, 0.025332313030958176, 0.000523572729434818, -0.040293797850608826, 0.01621188595890999, 0.05538344755768776, 0.005024864338338375, -0.006692908704280853, -0.02287977561354637, 0.008554075844585896, -0.016821755096316338, 0.00909153837710619, 0.011109458282589912, 0.05761316418647766, -0.011258399114012718, -0.0222606398165226, -0.03724260255694389, -0.01443271990865469, -0.021500933915376663, -0.02681347355246544, 0.0619514100253582, 0.05471019074320793, 0.004599235951900482, 0.0037776592653244734, 0.005907001439481974, -0.050644539296627045, -0.017595963552594185, -0.016164520755410194, -0.04300913214683533, -0.0022599627263844013, 0.012849701568484306, 0.00902070477604866, 0.03007330372929573, -0.05327785015106201, -0.003539928700774908, -0.0010069196578115225, -0.01872294396162033, -0.026217946782708168, -0.02085021138191223, -0.05490080267190933, 0.009403159841895103, 0.018844449892640114, -0.043576307594776154, -0.04696603864431381, 0.017445772886276245, -0.03988707438111305, -0.043013300746679306, -0.04433439299464226, 0.004843658301979303, 0.01399286836385727, -0.037605591118335724, -0.036526959389448166, -0.017303723841905594, -0.014701828360557556, 0.013720318675041199, 0.043883614242076874, 0.009424255229532719, -0.023521902039647102, 0.025701353326439857, -0.0027067819610238075, -0.013467814773321152, -0.007907569408416748, -0.07537691295146942, -0.037713609635829926, -0.031602270901203156, -0.04229366034269333, 0.020953988656401634, -0.0636780858039856, -0.04505152255296707, -0.007371074985712767, -0.0029663078021258116, 0.018373051658272743, 0.05074615031480789, -0.02018069662153721, 0.014300733804702759, 0.050678350031375885, 0.044535908848047256, 0.09799119085073471, 0.018766239285469055, -0.07090733200311661, 0.0031072241254150867, -0.009479989297688007, 0.020670007914304733, 0.030642054975032806, -0.03126130625605583, -0.05849355459213257, -0.0010429053800180554, 0.014434020966291428, 0.04643643647432327, -0.07737945020198822, -0.04148992523550987, 0.027196601033210754, -0.034824203699827194, -0.017771635204553604, 0.07974428683519363, -0.042647022753953934, 0.008304392918944359, -0.01878955215215683, -0.04190314933657646, 0.05365718528628349, -0.015567105263471603, 0.006540629547089338, 0.01752413436770439, -0.016991976648569107, -0.006322893779724836, 0.002579212887212634, 0.006549127399921417, 0.02120681293308735, -0.03133956715464592, 0.00028196233324706554, 0.053628988564014435, -0.009929625317454338, 0.01098386850208044, -0.023842547088861465, 0.002027914859354496, -0.04799618944525719, 0.02784169837832451, 0.05123579129576683, 0.06166638806462288, -0.059552885591983795, 0.052270982414484024, 0.011396987363696098, -0.016199417412281036, -0.023446783423423767, 0.01938760280609131, -0.07013041526079178, -0.061196256428956985, 0.04349249601364136, -0.01461716927587986, 0.009700343012809753, 0.08143739402294159, -0.03873324766755104, -0.0014525782316923141, -0.036053694784641266, 0.039823487401008606, 0.026155488565564156, 0.013090166263282299, -0.048758864402770996, -0.006190973799675703, 0.05563288927078247, -0.017576467245817184, 0.0003768872411455959, 0.036343250423669815, 0.027354028075933456, -0.07065858691930771, -0.026834653690457344, 0.004708610940724611, 0.016854478046298027, -0.03586243838071823, -0.0761834904551506, 0.006947242189198732, 0.009922651574015617, 0.04052263870835304, 0.04222412034869194, 0.0019509870326146483, 0.02471182681620121, 0.009100810624659061, 0.0052371546626091, 0.012333329766988754, -0.008198970928788185, -0.018581194803118706, -0.00635085254907608, -0.010190702974796295, 0.0747072845697403, 0.02950492687523365, -0.012352303601801395, -0.026308145374059677, 0.017917316406965256, -0.02361520752310753, -0.02796904928982258, -0.029263019561767578, -0.03710246831178665, 0.019820893183350563, -0.061823997646570206, -0.020359046757221222, 0.026187442243099213, -0.018644435331225395, 0.03218809887766838, 0.02717410959303379, -0.028665656223893166, 0.0034949614200741053, 0.05402469262480736, -0.030074331909418106, -0.008827963843941689, 0.06436179578304291, -0.013673677109181881, 0.027364179491996765, -0.07622275501489639, -0.0008764177327975631, 0.0365106575191021, -0.08666625618934631]\n",
      "768\n",
      "‚úì Using Google Gemini Embeddings\n",
      "üí° Google Gemini embeddings are free but cost ~$0.02 per 1M tokens\n"
     ]
    }
   ],
   "source": [
    "##Use Gemini Embedding model\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "gemini_embeddings = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\")\n",
    "Test_query=\"What is the capital of France?\"\n",
    "Gemini_Embeddings=gemini_embeddings.embed_query(Test_query)\n",
    "print(Gemini_Embeddings)\n",
    "print(len(Gemini_Embeddings))\n",
    "print(\"‚úì Using Google Gemini Embeddings\")\n",
    "\n",
    "print(\"üí° Google Gemini embeddings are free but cost ~$0.02 per 1M tokens\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Step 4: Create Vector Store with Chroma üóÑÔ∏è\n",
    "\n",
    "Store embeddings in Chroma for fast similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating vector store...\n",
      "‚úì Vector store created with 5 chunks\n",
      "‚úì Persisted to: ./chroma_rag_db\n",
      "\n",
      "Test Query: 'What is RAG?'\n",
      "Top result: Retrieval Augmented Generation (RAG) is a technique that combines information retrieval \n",
      "        wit...\n"
     ]
    }
   ],
   "source": [
    "from langchain_chroma import Chroma\n",
    "import shutil\n",
    "\n",
    "# Clean up any existing database\n",
    "shutil.rmtree(\"./chroma_rag_db\", ignore_errors=True)\n",
    "\n",
    "# Create vector store\n",
    "print(\"Creating vector store...\")\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=embeddings,\n",
    "    #persist_directory=\"./chroma_rag_db\",  # Persist to disk\n",
    "    collection_name=\"rag_demo\"\n",
    ")\n",
    "\n",
    "print(f\"‚úì Vector store created with {len(splits)} chunks\")\n",
    "print(f\"‚úì Persisted to: ./chroma_rag_db\")\n",
    "\n",
    "# Test retrieval\n",
    "test_query = \"What is RAG?\"\n",
    "test_results = vectorstore.similarity_search(test_query, k=2)\n",
    "\n",
    "print(f\"\\nTest Query: '{test_query}'\")\n",
    "print(f\"Top result: {test_results[0].page_content[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üí° What just happened:**\n",
    "1. Each chunk was converted to a 384-dimensional vector\n",
    "2. Vectors stored in Chroma with metadata\n",
    "3. Chroma created an index for fast similarity search\n",
    "4. Data persisted to disk (can reload later!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Step 5: Create a Retriever üîç\n",
    "\n",
    "Retriever finds the most relevant chunks for a query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Retriever created with Similarity search\n",
      "  - Returns: 3 most relevant chunks\n",
      "\n",
      "Test Query: 'What are React Agents?'\n",
      "Retrieved 3 chunks:\n",
      "\n",
      "1. Retrieval Augmented Generation (RAG) is a technique that combines information re...\n",
      "   Source: rag_explanation.txt\n",
      "\n",
      "2. Chroma is an open-source embedding database designed for AI applications. \n",
      "     ...\n",
      "   Source: chroma_db.txt\n",
      "\n",
      "3. LangChain is a framework for developing applications powered by language models....\n",
      "   Source: langchain_intro.txt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create retriever from vector store\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",  # Maximum Marginal Relevance (diverse results)\n",
    "    search_kwargs={\n",
    "        \"k\": 3,          # Return top 3 chunks\n",
    "        #\"fetch_k\": 10    # Fetch 10 candidates for MMR selection\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"‚úì Retriever created with Similarity search\")\n",
    "print(\"  - Returns: 3 most relevant chunks\")\n",
    "#print(\"  - Strategy: MMR (diverse, non-redundant results)\")\n",
    "\n",
    "# Test retrieval\n",
    "test_query = \"What are React Agents?\"\n",
    "retrieved_docs = retriever.invoke(test_query)\n",
    "#retrieved_docs = retriever.get_relevant_documents(test_query)\n",
    "\n",
    "print(f\"\\nTest Query: '{test_query}'\")\n",
    "print(f\"Retrieved {len(retrieved_docs)} chunks:\\n\")\n",
    "for i, doc in enumerate(retrieved_docs, 1):\n",
    "    print(f\"{i}. {doc.page_content[:80]}...\")\n",
    "    print(f\"   Source: {doc.metadata.get('source', 'N/A')}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üí° Why MMR?**\n",
    "- Standard search might return 3 very similar chunks\n",
    "- MMR balances relevance + diversity\n",
    "- Result: Better coverage, less redundancy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Step 6: Set Up the LLM ü§ñ\n",
    "\n",
    "Choose your LLM: OpenAI (paid) or HuggingFace (free)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option A: OpenAI (Best Quality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'OPENAI_API_KEY' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_openai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatOpenAI\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Ensure the API key is set as an environment variable if not already\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mOPENAI_API_KEY\u001b[49m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.getenv(\u001b[33m\"\u001b[39m\u001b[33mOPENAI_API_KEY\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m      5\u001b[39m     os.environ[\u001b[33m\"\u001b[39m\u001b[33mOPENAI_API_KEY\u001b[39m\u001b[33m\"\u001b[39m] = OPENAI_API_KEY\n\u001b[32m      7\u001b[39m model = ChatOpenAI(\n\u001b[32m      8\u001b[39m     model=\u001b[33m\"\u001b[39m\u001b[33mgpt-4o-mini\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      9\u001b[39m    temperature=\u001b[32m0\u001b[39m  \u001b[38;5;66;03m# 0 = focused, factual responses\u001b[39;00m\n\u001b[32m     10\u001b[39m )\n",
      "\u001b[31mNameError\u001b[39m: name 'OPENAI_API_KEY' is not defined"
     ]
    }
   ],
   "source": [
    "# Uncomment if you have OpenAI API key\n",
    "from langchain_openai import ChatOpenAI\n",
    "# Ensure the API key is set as an environment variable if not already\n",
    "if OPENAI_API_KEY and not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "\n",
    "model = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "   temperature=0  # 0 = focused, factual responses\n",
    ")\n",
    "print(\"‚úì Using OpenAI GPT-4o-mini\")\n",
    "\n",
    "print(\"üí° OpenAI provides best quality but costs ~$0.002 per request\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Option B : Google Gemini(Free)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash-lite\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash-lite\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0, model: gemini-2.0-flash-lite\n",
      "Please retry in 55.653125342s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash-lite\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "}\n",
      "violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash-lite\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "}\n",
      "violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_input_token_count\"\n",
      "  quota_id: \"GenerateContentInputTokensPerModelPerMinute-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash-lite\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 55\n",
      "}\n",
      "].\n"
     ]
    },
    {
     "ename": "ResourceExhausted",
     "evalue": "429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash-lite\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash-lite\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0, model: gemini-2.0-flash-lite\nPlease retry in 53.510868762s. [links {\n  description: \"Learn more about Gemini API quotas\"\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n}\n, violations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.0-flash-lite\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.0-flash-lite\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_input_token_count\"\n  quota_id: \"GenerateContentInputTokensPerModelPerMinute-FreeTier\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.0-flash-lite\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n}\n, retry_delay {\n  seconds: 53\n}\n]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mResourceExhausted\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_google_genai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatGoogleGenerativeAI\n\u001b[32m      4\u001b[39m llm_gemini = ChatGoogleGenerativeAI(model=\u001b[33m\"\u001b[39m\u001b[33mgemini-2.0-flash-lite\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mllm_gemini\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mHello, how are you?\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m‚úì Using Google Gemini Pro\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\RAG WorkingSession\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:395\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    383\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    384\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    385\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    390\u001b[39m     **kwargs: Any,\n\u001b[32m    391\u001b[39m ) -> BaseMessage:\n\u001b[32m    392\u001b[39m     config = ensure_config(config)\n\u001b[32m    393\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    394\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m395\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    396\u001b[39m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    397\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    398\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    400\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    405\u001b[39m     ).message\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\RAG WorkingSession\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1025\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m   1016\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   1017\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m   1018\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     **kwargs: Any,\n\u001b[32m   1023\u001b[39m ) -> LLMResult:\n\u001b[32m   1024\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m-> \u001b[39m\u001b[32m1025\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\RAG WorkingSession\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:842\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    840\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    841\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m842\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    844\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    845\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    846\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    847\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    848\u001b[39m         )\n\u001b[32m    849\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    850\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\RAG WorkingSession\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1091\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1089\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1090\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1091\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1092\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1093\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1094\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1095\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\RAG WorkingSession\\.venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:961\u001b[39m, in \u001b[36mChatGoogleGenerativeAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, tools, functions, safety_settings, tool_config, generation_config, cached_content, tool_choice, **kwargs)\u001b[39m\n\u001b[32m    935\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_generate\u001b[39m(\n\u001b[32m    936\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    937\u001b[39m     messages: List[BaseMessage],\n\u001b[32m   (...)\u001b[39m\u001b[32m    948\u001b[39m     **kwargs: Any,\n\u001b[32m    949\u001b[39m ) -> ChatResult:\n\u001b[32m    950\u001b[39m     request = \u001b[38;5;28mself\u001b[39m._prepare_request(\n\u001b[32m    951\u001b[39m         messages,\n\u001b[32m    952\u001b[39m         stop=stop,\n\u001b[32m   (...)\u001b[39m\u001b[32m    959\u001b[39m         tool_choice=tool_choice,\n\u001b[32m    960\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m961\u001b[39m     response: GenerateContentResponse = \u001b[43m_chat_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    962\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    963\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    964\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgeneration_method\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    965\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdefault_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    966\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    967\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _response_to_result(response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\RAG WorkingSession\\.venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:196\u001b[39m, in \u001b[36m_chat_with_retry\u001b[39m\u001b[34m(generation_method, **kwargs)\u001b[39m\n\u001b[32m    193\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    194\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m--> \u001b[39m\u001b[32m196\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_chat_with_retry\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\RAG WorkingSession\\.venv\\Lib\\site-packages\\tenacity\\__init__.py:338\u001b[39m, in \u001b[36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[39m\u001b[34m(*args, **kw)\u001b[39m\n\u001b[32m    336\u001b[39m copy = \u001b[38;5;28mself\u001b[39m.copy()\n\u001b[32m    337\u001b[39m wrapped_f.statistics = copy.statistics  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m338\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\RAG WorkingSession\\.venv\\Lib\\site-packages\\tenacity\\__init__.py:477\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    475\u001b[39m retry_state = RetryCallState(retry_object=\u001b[38;5;28mself\u001b[39m, fn=fn, args=args, kwargs=kwargs)\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m477\u001b[39m     do = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    478\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    479\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\RAG WorkingSession\\.venv\\Lib\\site-packages\\tenacity\\__init__.py:378\u001b[39m, in \u001b[36mBaseRetrying.iter\u001b[39m\u001b[34m(self, retry_state)\u001b[39m\n\u001b[32m    376\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    377\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.actions:\n\u001b[32m--> \u001b[39m\u001b[32m378\u001b[39m     result = \u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\RAG WorkingSession\\.venv\\Lib\\site-packages\\tenacity\\__init__.py:420\u001b[39m, in \u001b[36mBaseRetrying._post_stop_check_actions.<locals>.exc_check\u001b[39m\u001b[34m(rs)\u001b[39m\n\u001b[32m    418\u001b[39m retry_exc = \u001b[38;5;28mself\u001b[39m.retry_error_cls(fut)\n\u001b[32m    419\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.reraise:\n\u001b[32m--> \u001b[39m\u001b[32m420\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mretry_exc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    421\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m retry_exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfut\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexception\u001b[39;00m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\RAG WorkingSession\\.venv\\Lib\\site-packages\\tenacity\\__init__.py:187\u001b[39m, in \u001b[36mRetryError.reraise\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    185\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mreraise\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> t.NoReturn:\n\u001b[32m    186\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.last_attempt.failed:\n\u001b[32m--> \u001b[39m\u001b[32m187\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlast_attempt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    188\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\uv\\python\\cpython-3.11.11-windows-x86_64-none\\Lib\\concurrent\\futures\\_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\uv\\python\\cpython-3.11.11-windows-x86_64-none\\Lib\\concurrent\\futures\\_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\RAG WorkingSession\\.venv\\Lib\\site-packages\\tenacity\\__init__.py:480\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    478\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    479\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m480\u001b[39m         result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    481\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[32m    482\u001b[39m         retry_state.set_exception(sys.exc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\RAG WorkingSession\\.venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:194\u001b[39m, in \u001b[36m_chat_with_retry.<locals>._chat_with_retry\u001b[39m\u001b[34m(**kwargs)\u001b[39m\n\u001b[32m    190\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ChatGoogleGenerativeAIError(\n\u001b[32m    191\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvalid argument provided to Gemini: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    192\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    193\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m194\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\RAG WorkingSession\\.venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:178\u001b[39m, in \u001b[36m_chat_with_retry.<locals>._chat_with_retry\u001b[39m\u001b[34m(**kwargs)\u001b[39m\n\u001b[32m    175\u001b[39m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_chat_with_retry\u001b[39m(**kwargs: Any) -> Any:\n\u001b[32m    177\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m178\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgeneration_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    179\u001b[39m     \u001b[38;5;66;03m# Do not retry for these errors.\u001b[39;00m\n\u001b[32m    180\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m google.api_core.exceptions.FailedPrecondition \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\RAG WorkingSession\\.venv\\Lib\\site-packages\\google\\ai\\generativelanguage_v1beta\\services\\generative_service\\client.py:835\u001b[39m, in \u001b[36mGenerativeServiceClient.generate_content\u001b[39m\u001b[34m(self, request, model, contents, retry, timeout, metadata)\u001b[39m\n\u001b[32m    832\u001b[39m \u001b[38;5;28mself\u001b[39m._validate_universe_domain()\n\u001b[32m    834\u001b[39m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m835\u001b[39m response = \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    836\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    837\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    838\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    839\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    840\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    842\u001b[39m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n\u001b[32m    843\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\RAG WorkingSession\\.venv\\Lib\\site-packages\\google\\api_core\\gapic_v1\\method.py:131\u001b[39m, in \u001b[36m_GapicCallable.__call__\u001b[39m\u001b[34m(self, timeout, retry, compression, *args, **kwargs)\u001b[39m\n\u001b[32m    128\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    129\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mcompression\u001b[39m\u001b[33m\"\u001b[39m] = compression\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\RAG WorkingSession\\.venv\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:294\u001b[39m, in \u001b[36mRetry.__call__.<locals>.retry_wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    290\u001b[39m target = functools.partial(func, *args, **kwargs)\n\u001b[32m    291\u001b[39m sleep_generator = exponential_sleep_generator(\n\u001b[32m    292\u001b[39m     \u001b[38;5;28mself\u001b[39m._initial, \u001b[38;5;28mself\u001b[39m._maximum, multiplier=\u001b[38;5;28mself\u001b[39m._multiplier\n\u001b[32m    293\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m294\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    297\u001b[39m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m=\u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    300\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\RAG WorkingSession\\.venv\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:156\u001b[39m, in \u001b[36mretry_target\u001b[39m\u001b[34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[39m\n\u001b[32m    152\u001b[39m \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[32m    153\u001b[39m \u001b[38;5;66;03m# This function explicitly must deal with broad exceptions.\u001b[39;00m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    155\u001b[39m     \u001b[38;5;66;03m# defer to shared logic for handling errors\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m     next_sleep = \u001b[43m_retry_error_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    157\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    158\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdeadline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[43m        \u001b[49m\u001b[43msleep_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[43m        \u001b[49m\u001b[43merror_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpredicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m        \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexception_factory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# if exception not raised, sleep before next attempt\u001b[39;00m\n\u001b[32m    167\u001b[39m     time.sleep(next_sleep)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\RAG WorkingSession\\.venv\\Lib\\site-packages\\google\\api_core\\retry\\retry_base.py:214\u001b[39m, in \u001b[36m_retry_error_helper\u001b[39m\u001b[34m(exc, deadline, sleep_iterator, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[39m\n\u001b[32m    208\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m predicate_fn(exc):\n\u001b[32m    209\u001b[39m     final_exc, source_exc = exc_factory_fn(\n\u001b[32m    210\u001b[39m         error_list,\n\u001b[32m    211\u001b[39m         RetryFailureReason.NON_RETRYABLE_ERROR,\n\u001b[32m    212\u001b[39m         original_timeout,\n\u001b[32m    213\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m214\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m final_exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msource_exc\u001b[39;00m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m on_error_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    216\u001b[39m     on_error_fn(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\RAG WorkingSession\\.venv\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:147\u001b[39m, in \u001b[36mretry_target\u001b[39m\u001b[34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[39m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    146\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m         result = \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    148\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m inspect.isawaitable(result):\n\u001b[32m    149\u001b[39m             warnings.warn(_ASYNC_RETRY_WARNING)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\RAG WorkingSession\\.venv\\Lib\\site-packages\\google\\api_core\\timeout.py:130\u001b[39m, in \u001b[36mTimeToDeadlineTimeout.__call__.<locals>.func_with_timeout\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    126\u001b[39m         remaining_timeout = \u001b[38;5;28mself\u001b[39m._timeout\n\u001b[32m    128\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m] = remaining_timeout\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\RAG WorkingSession\\.venv\\Lib\\site-packages\\google\\api_core\\grpc_helpers.py:77\u001b[39m, in \u001b[36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m callable_(*args, **kwargs)\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m grpc.RpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions.from_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n",
      "\u001b[31mResourceExhausted\u001b[39m: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash-lite\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash-lite\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0, model: gemini-2.0-flash-lite\nPlease retry in 53.510868762s. [links {\n  description: \"Learn more about Gemini API quotas\"\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n}\n, violations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.0-flash-lite\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.0-flash-lite\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_input_token_count\"\n  quota_id: \"GenerateContentInputTokensPerModelPerMinute-FreeTier\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.0-flash-lite\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n}\n, retry_delay {\n  seconds: 53\n}\n]"
     ]
    }
   ],
   "source": [
    "#google gemini\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# Ensure the API key is set as an environment variable if not already\n",
    "if GEMINI_API_KEY and not os.getenv(\"GEMINI_API_KEY\"):\n",
    "    os.environ[\"GEMINI_API_KEY\"] = GEMINI_API_KEY\n",
    "\n",
    "llm_gemini = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash-lite\")\n",
    "print(llm_gemini.invoke(\"Hello, how are you?\"))\n",
    "print(\"‚úì Using Google Gemini Pro\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option C: Using Groq Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, the user said Hello, how are you? which is a greeting. I should respond with a friendly greeting as well. I need to acknowledge their greeting and offer assistance. Maybe add an emoji to keep it warm. I should keep it simple and open-ended to encourage them to ask their question or share what's on their mind. Let me check for any cultural nuances or tone issues. Yep, that sounds good. Ready to respond.\n",
      "</think>\n",
      "\n",
      "Hello! üòä I'm here and ready to help. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "# Ensure the API key is set as an environment variable if not already\n",
    "if GROQ_API_KEY and not os.getenv(\"GROQ_API_KEY\"):\n",
    "    os.environ[\"GROQ_API_KEY\"] = GROQ_API_KEY\n",
    "\n",
    "chat_groq = ChatGroq(model_name=\"qwen/qwen3-32b\")\n",
    "response = chat_groq.invoke(\"Hello, how are you?\")\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üí° Note:** HuggingFace free tier has rate limits. For production, use OpenAI or host your own model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Step 7: Build the RAG Chain üîó\n",
    "\n",
    "This is where the magic happens! Connect retriever + LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì RAG Chain created successfully!\n",
      "  - Retriever: Finds relevant chunks\n",
      "  - LLM: Generates answer from chunks\n",
      "  - Returns: Answer + source documents\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA \n",
    "#from langchain.chains import create_retrieval_chain\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Create a custom prompt\n",
    "prompt_template = \"\"\"Use the following pieces of context to answer the question at the end. \n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer: Let me help you with that.\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, \n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "# Create the RAG chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=chat_groq,\n",
    "    chain_type=\"stuff\",  # \"stuff\" = put all retrieved docs into prompt\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,  # Return sources with answer\n",
    "    chain_type_kwargs={\"prompt\": PROMPT}\n",
    ")\n",
    "\n",
    "print(\"‚úì RAG Chain created successfully!\")\n",
    "print(\"  - Retriever: Finds relevant chunks\")\n",
    "print(\"  - LLM: Generates answer from chunks\")\n",
    "print(\"  - Returns: Answer + source documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üí° What's happening in the chain:**\n",
    "1. User asks question\n",
    "2. Retriever finds 3 most relevant chunks\n",
    "3. Chunks inserted into prompt as \"context\"\n",
    "4. LLM generates answer using that context\n",
    "5. Sources returned for verification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Step 8: Test Your RAG ChatBot! üéâ\n",
    "\n",
    "Let's ask some questions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Question: How does RAG work?\n",
      "======================================================================\n",
      "\n",
      "Answer:\n",
      "<think>\n",
      "Okay, the user is asking how RAG works. Let me start by recalling the context provided.\n",
      "\n",
      "First, the context mentions that RAG combines information retrieval with text generation. The process involves two main steps: retrieving documents and then generating a response. The user wants a clear explanation of how this works.\n",
      "\n",
      "I need to break it down step by step. So, RAG starts by retrieving relevant documents from a knowledge base. That's the retrieval part. Then, these documents are passed to a language model to generate the answer. This helps prevent hallucinations and allows the model to use up-to-date information.\n",
      "\n",
      "I should mention the importance of document splitting, like using the RecursiveCharacterTextSplitter. Optimal chunk sizes are 500-1000 characters with some overlap. But wait, the question is about how RAG works in general, so maybe the splitting details are part of the retrieval process? Or maybe they're a separate preprocessing step. The context says document splitting is crucial for RAG systems, so maybe I should include that as part of the setup before the retrieval.\n",
      "\n",
      "Also, Chroma is mentioned as an embedding database used for storing the documents. It's important for the similarity search part. So during retrieval, the system uses Chroma to find similar documents based on the query's embeddings.\n",
      "\n",
      "Putting it all together: RAG works by first splitting documents into chunks, creating embeddings with Chroma, then when a query comes in, it retrieves the most relevant chunks using similarity search, and then the language model generates the answer using those retrieved chunks as context. This ensures the answers are accurate and based on the provided knowledge base.\n",
      "\n",
      "Wait, the user might not need the details about Chroma unless it's directly part of how RAG functions. The core of RAG is retrieval + generation. The document splitting and Chroma are supporting components. So the main answer should focus on the two-step process: retrieval followed by generation. The other details can be mentioned as supporting elements that make RAG effective, like preventing hallucinations and using up-to-date info.\n",
      "\n",
      "Let me structure the answer: Start with the two main steps (retrieve, generate), mention the role of document splitting and chunking to ensure effective retrieval, and note that using an embedding database like Chroma helps in fast similarity search. Then conclude with the benefits like accurate and grounded responses without hallucinations.\n",
      "</think>\n",
      "\n",
      "Answer: Retrieval Augmented Generation (RAG) works by first retrieving relevant documents from a knowledge base using techniques like similarity search (often with an embedding database such as Chroma). These retrieved documents are then passed to a language model, which generates a response grounded in the retrieved information. This two-step process ensures answers are accurate, contextually relevant, and based on up-to-date or specific data, while reducing hallucinations. Document splitting (e.g., into 500‚Äì1000-character chunks with overlap) is critical to optimize retrieval effectiveness and maintain contextual coherence.\n",
      "\n",
      "======================================================================\n",
      "Sources:\n",
      "\n",
      "1. rag_explanation.txt\n",
      "   Retrieval Augmented Generation (RAG) is a technique that combines information retrieval \n",
      "        with text generation. It works by first retrieving re...\n",
      "\n",
      "2. splitting_guide.txt\n",
      "   Document splitting is crucial for RAG systems. The RecursiveCharacterTextSplitter \n",
      "        is recommended as it intelligently splits on paragraphs, se...\n",
      "\n",
      "3. chroma_db.txt\n",
      "   Chroma is an open-source embedding database designed for AI applications. \n",
      "        It provides fast similarity search, easy integration with LangChain...\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "def ask_question(question):\n",
    "    \"\"\"Ask a question and display answer with sources.\"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    result = qa_chain.invoke({\"query\": question})\n",
    "    \n",
    "    print(f\"Answer:\\n{result['result']}\\n\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Sources:\")\n",
    "    for i, doc in enumerate(result['source_documents'], 1):\n",
    "        print(f\"\\n{i}. {doc.metadata.get('source', 'Unknown')}\")\n",
    "        print(f\"   {doc.page_content[:150]}...\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "# Test with sample questions\n",
    "questions = [\n",
    "    \"What are the characteristics of React Agent?\",\n",
    "    \"How does RAG work?\",\n",
    "    \"What embedding models are commonly used?\",\n",
    "    \"Why is document splitting important?\"\n",
    "]\n",
    "\n",
    "# Ask the first question\n",
    "ask_question(questions[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try more questions\n",
    "ask_question(questions[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Question: What is Chroma and why use it?\n",
      "======================================================================\n",
      "\n",
      "Answer:\n",
      "Chroma is an open-source embedding database designed for AI applications.\n",
      "\n",
      "You should use it because it provides:\n",
      "*   Fast similarity search\n",
      "*   Easy integration with LangChain\n",
      "*   Persistent storage\n",
      "*   Support for metadata filtering\n",
      "*   It is perfect for building RAG (Retrieval Augmented Generation) applications.\n",
      "*   It can run locally or in client-server mode.\n",
      "\n",
      "======================================================================\n",
      "Sources:\n",
      "\n",
      "1. chroma_db.txt\n",
      "   Chroma is an open-source embedding database designed for AI applications. \n",
      "        It provides fast similarity search, easy integration with LangChain...\n",
      "\n",
      "2. langchain_intro.txt\n",
      "   LangChain is a framework for developing applications powered by language models. \n",
      "        It enables applications that are context-aware and can reaso...\n",
      "\n",
      "3. splitting_guide.txt\n",
      "   Document splitting is crucial for RAG systems. The RecursiveCharacterTextSplitter \n",
      "        is recommended as it intelligently splits on paragraphs, se...\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Ask your own question!\n",
    "custom_question = \"What is Chroma and why use it?\"  # Change this!\n",
    "ask_question(custom_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Question: Who is Donald Trump?\n",
      "======================================================================\n",
      "\n",
      "Answer:\n",
      "Let me help you with that. Based on the provided context, I don't have information about who Donald Trump is. The context discusses Chroma, Retrieval Augmented Generation (RAG), and vector embeddings.\n",
      "\n",
      "======================================================================\n",
      "Sources:\n",
      "\n",
      "1. chroma_db.txt\n",
      "   Chroma is an open-source embedding database designed for AI applications. \n",
      "        It provides fast similarity search, easy integration with LangChain...\n",
      "\n",
      "2. rag_explanation.txt\n",
      "   Retrieval Augmented Generation (RAG) is a technique that combines information retrieval \n",
      "        with text generation. It works by first retrieving re...\n",
      "\n",
      "3. embeddings_guide.txt\n",
      "   Vector embeddings are numerical representations of text that capture semantic meaning. \n",
      "        Similar texts produce similar embedding vectors. Commo...\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Ask your own question!\n",
    "custom_question = \"Who is Donald Trump?\"  # Change this!\n",
    "ask_question(custom_question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üí° Notice:**\n",
    "- ‚úÖ Answers are grounded in your documents\n",
    "- ‚úÖ Sources are provided for verification\n",
    "- ‚úÖ No hallucination (answers \"I don't know\" when info not in docs)\n",
    "\n",
    "**This is the power of RAG!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Step 9: Interactive Chat Interface üí¨\n",
    "\n",
    "Let's create a simple chat loop for continuous interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "RAG ChatBot Ready! Ask me anything about the documents.\n",
      "Type 'quit' or 'exit' to end the conversation.\n",
      "======================================================================\n",
      "\n",
      "\n",
      "Bot: The characteristics of tool-enhanced agents include:\n",
      "\n",
      "1. **Intelligence**: They combine reasoning with a specialized tool loop to achieve desired outcomes.\n",
      "2. **Tool Selection**: After the initial reasoning phase, they select the appropriate tool for the task.\n",
      "3. **Tool Execution**: They execute the selected tool and repeat the cycle until the desired outcome is achieved.\n",
      "\n",
      "Examples of tool-enhanced agents include code generation tools like GitHub CoPilot, data analysis bots that combine multiple APIs, and automation tools.\n",
      "\n",
      "üìö Sources: data/Mastering AI Agents.pdf, data/Mastering AI Agents.pdf, data/Mastering AI Agents.pdf\n",
      "\n",
      "Bot: Self-reflecting agents are AI systems that incorporate meta-cognition, allowing them to think about their own thinking. They analyze their reasoning, assess their decisions, and learn from mistakes, which enables them to solve tasks, explain their reasoning, and improve over time. This continuous loop of thinking, doing, and learning enhances their reliability and accountability.\n",
      "\n",
      "üìö Sources: data/Mastering AI Agents.pdf, data/Mastering AI Agents.pdf, data/Mastering AI Agents.pdf\n",
      "\n",
      "Bot: AI agents are systems designed to perform specific tasks autonomously or semi-autonomously, often utilizing artificial intelligence to enhance their capabilities. They can operate in various domains, such as customer support, where they handle queries and provide real-time assistance, or in research and data analysis, where they gather and analyze data to provide insights. The use of AI agents can improve efficiency and customer experience by delivering timely responses and allowing human staff to focus on more complex issues.\n",
      "\n",
      "üìö Sources: data/Mastering AI Agents.pdf, data/Mastering AI Agents.pdf, data/Mastering AI Agents.pdf\n",
      "\n",
      "Bot: I don't know.\n",
      "\n",
      "üìö Sources: data/Mastering AI Agents.pdf, data/Mastering AI Agents.pdf, data/Mastering AI Agents.pdf\n",
      "\n",
      "Bot: ReAct agents are intelligent systems that combine reasoning and action to mimic human problem-solving. They handle multi-step workflows by breaking them down into smaller, actionable parts and can dynamically adjust their strategies based on new data. ReAct agents assist with basic open-ended problem-solving, even when there is no direct solution path. Additionally, there are enhanced versions known as ReAct + RAG agents, which integrate real-time access to external knowledge sources, allowing for informed decision-making in high-stakes or precision-critical tasks.\n",
      "\n",
      "üìö Sources: data/Mastering AI Agents.pdf, data/Mastering AI Agents.pdf, data/Mastering AI Agents.pdf\n",
      "\n",
      "Bot: To define the ReAct agent, you can create a new Jupyter Notebook named `agent.ipynb` within your IDE of choice. In this notebook, you can import a prebuilt ReAct agent along with a web search tool called Tavily. The ReAct agent is characterized by its ability to mimic human problem-solving through reasoning and action, handling multi-step workflows, and dynamically adjusting its strategy based on new data. It operates in a cycle between reasoning and action phases, allowing it to continuously analyze and adapt until the desired outcome is achieved.\n",
      "\n",
      "üìö Sources: data/Mastering AI Agents.pdf, data/Mastering AI Agents.pdf, data/Mastering AI Agents.pdf\n",
      "\n",
      "Bot: I don't know.\n",
      "\n",
      "üìö Sources: data/Mastering AI Agents.pdf, data/Mastering AI Agents.pdf, data/Mastering AI Agents.pdf\n",
      "\n",
      "Goodbye! üëã\n",
      "üí° Uncomment 'chat_loop()' above to start interactive chat!\n"
     ]
    }
   ],
   "source": [
    "def chat_loop():\n",
    "    \"\"\"\n",
    "    Simple chat interface.\n",
    "    Type 'quit' or 'exit' to end.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"RAG ChatBot Ready! Ask me anything about the documents.\")\n",
    "    print(\"Type 'quit' or 'exit' to end the conversation.\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    while True:\n",
    "        # Get user input\n",
    "        question = input(\"\\nYou: \").strip()\n",
    "        \n",
    "        # Check for exit\n",
    "        if question.lower() in ['quit', 'exit', 'q']:\n",
    "            print(\"\\nGoodbye! üëã\")\n",
    "            break\n",
    "        \n",
    "        if not question:\n",
    "            continue\n",
    "        \n",
    "        # Get answer\n",
    "        try:\n",
    "            result = qa_chain.invoke({\"query\": question})\n",
    "            print(f\"\\nBot: {result['result']}\")\n",
    "            print(f\"\\nüìö Sources: {', '.join([doc.metadata.get('source', 'N/A') for doc in result['source_documents']])}\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError: {e}\")\n",
    "            print(\"Please try again.\")\n",
    "\n",
    "# Uncomment to start chat\n",
    "chat_loop()\n",
    "\n",
    "print(\"üí° Uncomment 'chat_loop()' above to start interactive chat!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Bonus: Simple Streamlit UI üé®\n",
    "\n",
    "Want a web interface? Create `app.py` with this code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "streamlit_code = '''\n",
    "import streamlit as st\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings, HuggingFaceEndpoint\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Page config\n",
    "st.set_page_config(page_title=\"RAG ChatBot\", page_icon=\"ü§ñ\")\n",
    "st.title(\"ü§ñ RAG ChatBot\")\n",
    "st.caption(\"Ask questions about your documents!\")\n",
    "\n",
    "# Load components (cached)\n",
    "@st.cache_resource\n",
    "def load_rag_chain():\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "    vectorstore = Chroma(\n",
    "        persist_directory=\"./chroma_rag_db\",\n",
    "        embedding_function=embeddings,\n",
    "        collection_name=\"rag_demo\"\n",
    "    )\n",
    "    retriever = vectorstore.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 3})\n",
    "    llm = HuggingFaceEndpoint(repo_id=\"mistralai/Mistral-7B-Instruct-v0.2\", temperature=0.1)\n",
    "    \n",
    "    return RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        retriever=retriever,\n",
    "        return_source_documents=True\n",
    "    )\n",
    "\n",
    "qa_chain = load_rag_chain()\n",
    "\n",
    "# Chat interface\n",
    "if \"messages\" not in st.session_state:\n",
    "    st.session_state.messages = []\n",
    "\n",
    "# Display chat history\n",
    "for message in st.session_state.messages:\n",
    "    with st.chat_message(message[\"role\"]):\n",
    "        st.write(message[\"content\"])\n",
    "\n",
    "# User input\n",
    "if prompt := st.chat_input(\"Ask a question...\"):\n",
    "    # Add user message\n",
    "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "    with st.chat_message(\"user\"):\n",
    "        st.write(prompt)\n",
    "    \n",
    "    # Get response\n",
    "    with st.chat_message(\"assistant\"):\n",
    "        with st.spinner(\"Thinking...\"):\n",
    "            result = qa_chain.invoke({\"query\": prompt})\n",
    "            response = result[\"result\"]\n",
    "            sources = [doc.metadata.get(\"source\", \"N/A\") for doc in result[\"source_documents\"]]\n",
    "            \n",
    "            st.write(response)\n",
    "            st.caption(f\"üìö Sources: {', '.join(set(sources))}\")\n",
    "    \n",
    "    # Add assistant message\n",
    "    st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "'''\n",
    "\n",
    "# Save to file\n",
    "with open('streamlit_app.py', 'w') as f:\n",
    "    f.write(streamlit_code)\n",
    "\n",
    "print(\"‚úì Created streamlit_app.py\")\n",
    "print(\"\\nTo run:\")\n",
    "print(\"  pip install streamlit\")\n",
    "print(\"  streamlit run streamlit_app.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# What You've Built! üéâ\n",
    "\n",
    "**Congratulations!** You now have a fully functional RAG chatbot that:\n",
    "\n",
    "‚úÖ **Loads documents** from multiple sources (PDF, web, text)\n",
    "‚úÖ **Splits intelligently** using RecursiveCharacterTextSplitter\n",
    "‚úÖ **Creates embeddings** using HuggingFace (free!)\n",
    "‚úÖ **Stores in Chroma** with persistent storage\n",
    "‚úÖ **Retrieves with MMR** for diverse, relevant results\n",
    "‚úÖ **Generates answers** using LLM (Mistral or GPT)\n",
    "‚úÖ **Shows sources** for verification\n",
    "‚úÖ **Has a UI** (Streamlit)\n",
    "\n",
    "**All in ~30 minutes!**\n",
    "\n",
    "---\n",
    "\n",
    "## Performance & Cost\n",
    "\n",
    "**With Free Options (HuggingFace):**\n",
    "- üí∞ Cost: $0\n",
    "- ‚ö° Speed: 2-5 seconds per query\n",
    "- üìä Quality: Good for most use cases\n",
    "\n",
    "**With OpenAI:**\n",
    "- üí∞ Cost: ~$0.002 per query\n",
    "- ‚ö° Speed: <1 second per query\n",
    "- üìä Quality: Excellent\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps üöÄ\n",
    "\n",
    "**You can now:**\n",
    "1. Replace sample docs with YOUR documents\n",
    "2. Adjust chunk size/overlap for your use case\n",
    "3. Try different embedding models\n",
    "4. Experiment with retrieval parameters\n",
    "5. Customize the prompt for your domain\n",
    "\n",
    "**In Part 5b (Coming Next):**\n",
    "- üî• Conversational RAG (chat history)\n",
    "- üî• Advanced retrieval (re-ranking, hybrid search)\n",
    "- üî• Evaluation metrics (measure quality)\n",
    "- üî• Production optimization (caching, streaming)\n",
    "- üî• Error handling & edge cases\n",
    "\n",
    "---\n",
    "\n",
    "## Common Issues & Solutions\n",
    "\n",
    "**Problem: \"Rate limit exceeded\" with HuggingFace**\n",
    "- Solution: Use OpenAI or host model locally with Ollama\n",
    "\n",
    "**Problem: \"Slow responses\"**\n",
    "- Solution: Use GPU for embeddings, try smaller chunks\n",
    "\n",
    "**Problem: \"Answers not accurate\"**\n",
    "- Solution: Adjust chunk size, try k=5 instead of k=3, improve prompts\n",
    "\n",
    "**Problem: \"Can't find answer in documents\"**\n",
    "- Solution: Check if info is actually in docs, try similarity search directly\n",
    "\n",
    "---\n",
    "\n",
    "## Resources\n",
    "\n",
    "- [LangChain Documentation](https://python.langchain.com/)\n",
    "- [Chroma Documentation](https://docs.trychroma.com/)\n",
    "- [HuggingFace Models](https://huggingface.co/models)\n",
    "- [My Previous Blogs](#) - Parts 1-4 of this series\n",
    "\n",
    "---\n",
    "\n",
    "# Thank You! üôè\n",
    "\n",
    "**You've completed the journey:**\n",
    "1. ‚úÖ Document Loading\n",
    "2. ‚úÖ Document Splitting\n",
    "3. ‚úÖ Vector Embeddings\n",
    "4. ‚úÖ Vector Databases\n",
    "5. ‚úÖ **Building RAG ChatBot** ‚Üê You are here!\n",
    "\n",
    "**What's next?** Try it with YOUR documents and share your results!\n",
    "\n",
    "**Questions?** Drop them in the comments below!\n",
    "\n",
    "**Stay tuned for Part 5b** - Production RAG patterns! üöÄ\n",
    "\n",
    "#LangChain #RAG #AI #MachineLearning #Python #ChatBot #VectorDatabase #LLM"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
